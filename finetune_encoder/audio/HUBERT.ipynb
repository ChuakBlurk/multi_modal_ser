{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445b6748-52e1-4ba0-b190-8d4b58042068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.12)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.32.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.24.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install accelerate -U\n",
    "!pip install datasets\n",
    "!pip install scikit-learn\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06e5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification, Trainer, TrainingArguments\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "# sys.path.append(\"E:/university/FYT/repos/multi_modal_ser\")\n",
    "sys.path.append(\"/home/multi_modal_ser\")\n",
    "from utils.dataset import MMSERDataset\n",
    "from datasets import load_metric\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194655d9",
   "metadata": {},
   "source": [
    "### Log the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1279cc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"hubert_nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3949283d-a9dc-4c3b-9e55-c16aa1bc5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Sun Oct 15 16:37:09 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:81:00.0 Off |                  Off |\n",
      "| 46%   33C    P8              18W / 450W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:82:00.0 Off |                  Off |\n",
      "| 40%   33C    P8              29W / 450W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        On  | 00000000:C1:00.0 Off |                  Off |\n",
      "| 40%   35C    P8              17W / 450W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        On  | 00000000:C2:00.0 Off |                  Off |\n",
      "| 45%   32C    P8              20W / 450W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0e9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion ID:  [2. 1. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "# mmser_ds = torch.load(\"E:/datasets/preprocessed/dataset/mmser_ds.pt\")\n",
    "mmser_ds = torch.load(\"/home/mmser_ds.pt\")\n",
    "print(\"Emotion ID: \", mmser_ds.df_[\"emotion_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fd87b",
   "metadata": {},
   "source": [
    "### HUBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de5fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/hubert-large-ls960-ft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d123cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProcessedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, base_ds, pretrained_model):\n",
    "        self.base_ds = base_ds\n",
    "        self.processor = AutoProcessor.from_pretrained(pretrained_model)\n",
    "        self.__process__()\n",
    "        \n",
    "    def __process__(self):\n",
    "        self.input_values_list = []\n",
    "        self.attention_mask_list = []\n",
    "        for raw_audio in tqdm(self.base_ds.raw_list):\n",
    "            processed = self.processor(raw_audio, \n",
    "                                       sampling_rate=16000,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=300000,\n",
    "                                       truncation=True, \n",
    "                                      return_tensors=\"np\")\n",
    "            self.input_values_list.append(processed[\"input_values\"].squeeze())\n",
    "            self.attention_mask_list.append(processed[\"attention_mask\"].squeeze())\n",
    "    def __len__(self):\n",
    "        return len(self.base_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        base_dict = self.base_ds[idx]\n",
    "        base_dict[\"input_values\"] = self.input_values_list[idx]\n",
    "        base_dict[\"attention_mask\"] = self.attention_mask_list[idx]\n",
    "        del base_dict[\"audio\"]\n",
    "        return base_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2690485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5531/5531 [00:12<00:00, 456.57it/s]\n"
     ]
    }
   ],
   "source": [
    "AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "processed_ds = ProcessedDataset(mmser_ds, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82abced",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea498c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, HubertModel, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.config.use_weighted_layer_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15de142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CustomClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_model, num_labels=4):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model)\n",
    "        self.config = self.encoder.config\n",
    "        self.config.num_labels = num_labels\n",
    "        self.projector = nn.Linear(self.config.hidden_size, self.config.classifier_proj_size)\n",
    "        self.classifier = nn.Linear(self.config.classifier_proj_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_values,\n",
    "        attention_mask = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(\n",
    "                input_values,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "            \n",
    "            hidden_states = outputs[0]\n",
    "\n",
    "        hidden_states = self.projector(hidden_states)\n",
    "        if attention_mask is None:\n",
    "            pooled_output = hidden_states.mean(dim=1)\n",
    "        else:\n",
    "            padding_mask = self.encoder._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n",
    "            hidden_states[~padding_mask] = 0.0\n",
    "            pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return {\"logits\":logits}\n",
    "    \n",
    "model = CustomClassifier(MODEL_NAME, mmser_ds.df_[\"emotion_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "686ee7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = model(torch.Tensor(processed_ds[:2][\"input_values\"]).squeeze(), \n",
    "#                       torch.Tensor(processed_ds[:2][\"attention_mask\"]).squeeze()).last_hidden_state\n",
    "# print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0dce8",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6a61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_ = processed_ds.base_ds.df_\n",
    "sess_dict = meta_df_.groupby(\"session\").groups\n",
    "all_indices = set(meta_df_.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a87265",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_ds = {}\n",
    "for sess in sess_dict:\n",
    "    sess_ds[sess+\"_train\"] = Subset(processed_ds, \n",
    "                                    indices=list(all_indices-set(sess_dict[sess])))\n",
    "    sess_ds[sess+\"_test\"] = Subset(processed_ds, \n",
    "                                    indices=sess_dict[sess])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a70a0a",
   "metadata": {},
   "source": [
    "### Custom Trainer, Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7eab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").type(torch.LongTensor).to(device)\n",
    "        \n",
    "        input_values = inputs[\"input_values\"].to(device).to(torch.float32)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device).to(torch.float32)\n",
    "        outputs = model(input_values, \n",
    "                       attention_mask)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss() \n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "def weighted_acc(y_true, y_pred):\n",
    "    return np.sum((np.array(y_pred).ravel() == np.array(y_true).ravel()))*1.0/len(y_true)\n",
    "    \n",
    "def unweighted_acc(y_true, y_pred):\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    classes = np.unique(y_true)\n",
    "    classes_accuracies = np.zeros(classes.shape[0])\n",
    "    for num, cls in enumerate(classes):\n",
    "        classes_accuracies[num] = weighted_acc(y_true[y_true == cls], y_pred[y_true == cls])\n",
    "    return np.mean(classes_accuracies)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_ = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    acc_ = metric_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    \n",
    "    return {\"wa\":weighted_acc(labels, predictions), \n",
    "            \"ua\":unweighted_acc(labels, predictions),\n",
    "            \"f1\":f1_, \n",
    "            \"accuracy\":acc_}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a102497",
   "metadata": {},
   "source": [
    "### Set SESS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESS_ID = list(sess_dict.keys())[5]\n",
    "\n",
    "def build_ds(sess_id):\n",
    "    train_size = int(len(sess_ds[sess_id+\"_train\"])*0.75)\n",
    "    val_size = len(sess_ds[sess_id+\"_train\"])-train_size\n",
    "    train_set, val_set = torch.utils.data.random_split(sess_ds[sess_id+\"_train\"], [train_size, val_size])\n",
    "    test_set = sess_ds[sess_id+\"_test\"]\n",
    "\n",
    "    print(\"Train Samples:\", len(train_set))\n",
    "    print(\"Val Samples:\", len(val_set))\n",
    "    print(\"Test Samples:\", len(test_set))\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da42efb-8df8-4e27-ab41-10e5d22030e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 3687\n",
      "Val Samples: 1229\n",
      "Test Samples: 615\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = build_ds(SESS_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c33e97-d744-4878-8d1e-c256c164876e",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021f1d5f-153c-46a2-9511-afb78a3105dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SESS_ID = list(sess_dict.keys())[4]\n",
    "# output_dir=os.path.join(\"/home/multi_modal_ser/finetune_encoder/check_pts\", \"HUBERT\", SESS_ID, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\"))\n",
    "# model.load_state_dict(torch.load(output_dir+\"/pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1ed21-6b96-4eb2-9a57-19dfe67abf35",
   "metadata": {},
   "source": [
    "##### Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a198bfc4-f5e1-425b-b22f-bfebdba7461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomClassifier(\n",
       "  (encoder): HubertModel(\n",
       "    (feature_extractor): HubertFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): HubertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): HubertEncoderStableLayerNorm(\n",
       "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): HubertSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (classifier): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3760e071-5f5a-4272-9c32-df3157dd6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.projector.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91494113",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=os.path.join(\"/home/multi_modal_ser/finetune_encoder/check_pts\", \"HUBERT\", SESS_ID, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\"))\n",
    "\n",
    "training_args = TrainingArguments(output_dir,report_to=\"wandb\")\n",
    "training_args.remove_unused_columns=False\n",
    "training_args.per_device_train_batch_size=40\n",
    "training_args.per_device_eval_batch_size=20\n",
    "training_args.logging_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.eval_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.evaluation_strategy=\"steps\" \n",
    "training_args.logging_strategy=\"steps\"\n",
    "training_args.load_best_model_at_end=True,\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.learning_rate=1e-3\n",
    "training_args.num_train_epochs=50\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f9655-316b-4506-b265-346270c1d046",
   "metadata": {},
   "source": [
    "API: 2999b8f99f0f62b4f64c48a1c8be9a16945183e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60cd73ac-30b3-4864-a640-907ff0714bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ses03M\n"
     ]
    }
   ],
   "source": [
    "print(SESS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426910fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmmser\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/multi_modal_ser/finetune_encoder/audio/wandb/run-20231015_163740-f80irmsx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mmser/huggingface/runs/f80irmsx' target=\"_blank\">playful-fog-4</a></strong> to <a href='https://wandb.ai/mmser/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mmser/huggingface' target=\"_blank\">https://wandb.ai/mmser/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mmser/huggingface/runs/f80irmsx' target=\"_blank\">https://wandb.ai/mmser/huggingface/runs/f80irmsx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 58:52, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wa</th>\n",
       "      <th>Ua</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.274700</td>\n",
       "      <td>1.264385</td>\n",
       "      <td>0.379170</td>\n",
       "      <td>0.407492</td>\n",
       "      <td>0.305973</td>\n",
       "      <td>0.379170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.194100</td>\n",
       "      <td>1.190978</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>0.468925</td>\n",
       "      <td>0.443428</td>\n",
       "      <td>0.446705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.152200</td>\n",
       "      <td>1.196802</td>\n",
       "      <td>0.418226</td>\n",
       "      <td>0.463783</td>\n",
       "      <td>0.401987</td>\n",
       "      <td>0.418226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.115200</td>\n",
       "      <td>1.158123</td>\n",
       "      <td>0.480879</td>\n",
       "      <td>0.535188</td>\n",
       "      <td>0.458613</td>\n",
       "      <td>0.480879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.106200</td>\n",
       "      <td>1.148275</td>\n",
       "      <td>0.462164</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.451405</td>\n",
       "      <td>0.462164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.069400</td>\n",
       "      <td>1.063626</td>\n",
       "      <td>0.535395</td>\n",
       "      <td>0.554941</td>\n",
       "      <td>0.530542</td>\n",
       "      <td>0.535395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.056100</td>\n",
       "      <td>1.153248</td>\n",
       "      <td>0.462978</td>\n",
       "      <td>0.483646</td>\n",
       "      <td>0.439058</td>\n",
       "      <td>0.462978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.051600</td>\n",
       "      <td>1.036663</td>\n",
       "      <td>0.545159</td>\n",
       "      <td>0.563090</td>\n",
       "      <td>0.541243</td>\n",
       "      <td>0.545159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.051700</td>\n",
       "      <td>1.167652</td>\n",
       "      <td>0.465419</td>\n",
       "      <td>0.481944</td>\n",
       "      <td>0.433375</td>\n",
       "      <td>0.465419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.028600</td>\n",
       "      <td>1.157540</td>\n",
       "      <td>0.471928</td>\n",
       "      <td>0.510597</td>\n",
       "      <td>0.458094</td>\n",
       "      <td>0.471928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.997300</td>\n",
       "      <td>1.065516</td>\n",
       "      <td>0.524003</td>\n",
       "      <td>0.551822</td>\n",
       "      <td>0.513494</td>\n",
       "      <td>0.524003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>1.239651</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.498026</td>\n",
       "      <td>0.437205</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.964400</td>\n",
       "      <td>1.177567</td>\n",
       "      <td>0.475997</td>\n",
       "      <td>0.499640</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.475997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>1.096461</td>\n",
       "      <td>0.510985</td>\n",
       "      <td>0.546280</td>\n",
       "      <td>0.500136</td>\n",
       "      <td>0.510985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.932100</td>\n",
       "      <td>1.003060</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.593710</td>\n",
       "      <td>0.573296</td>\n",
       "      <td>0.577705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.022600</td>\n",
       "      <td>1.058792</td>\n",
       "      <td>0.558177</td>\n",
       "      <td>0.614917</td>\n",
       "      <td>0.542111</td>\n",
       "      <td>0.558177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.931300</td>\n",
       "      <td>0.955742</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.596718</td>\n",
       "      <td>0.572094</td>\n",
       "      <td>0.575264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.008000</td>\n",
       "      <td>1.091319</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>0.545341</td>\n",
       "      <td>0.497672</td>\n",
       "      <td>0.518308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>1.024614</td>\n",
       "      <td>0.558177</td>\n",
       "      <td>0.575986</td>\n",
       "      <td>0.553408</td>\n",
       "      <td>0.558177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>0.997268</td>\n",
       "      <td>0.568755</td>\n",
       "      <td>0.581538</td>\n",
       "      <td>0.563738</td>\n",
       "      <td>0.568755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>1.126358</td>\n",
       "      <td>0.513426</td>\n",
       "      <td>0.534515</td>\n",
       "      <td>0.476881</td>\n",
       "      <td>0.513426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.926900</td>\n",
       "      <td>1.173011</td>\n",
       "      <td>0.487388</td>\n",
       "      <td>0.525112</td>\n",
       "      <td>0.477412</td>\n",
       "      <td>0.487388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.954107</td>\n",
       "      <td>0.584215</td>\n",
       "      <td>0.614761</td>\n",
       "      <td>0.577738</td>\n",
       "      <td>0.584215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.886300</td>\n",
       "      <td>0.964541</td>\n",
       "      <td>0.602116</td>\n",
       "      <td>0.640534</td>\n",
       "      <td>0.590796</td>\n",
       "      <td>0.602116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.918146</td>\n",
       "      <td>0.609439</td>\n",
       "      <td>0.618793</td>\n",
       "      <td>0.604880</td>\n",
       "      <td>0.609439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.886900</td>\n",
       "      <td>1.153914</td>\n",
       "      <td>0.496338</td>\n",
       "      <td>0.532285</td>\n",
       "      <td>0.488275</td>\n",
       "      <td>0.496338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>0.997753</td>\n",
       "      <td>0.560618</td>\n",
       "      <td>0.589948</td>\n",
       "      <td>0.554230</td>\n",
       "      <td>0.560618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.925300</td>\n",
       "      <td>1.084704</td>\n",
       "      <td>0.541090</td>\n",
       "      <td>0.596566</td>\n",
       "      <td>0.524336</td>\n",
       "      <td>0.541090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.913816</td>\n",
       "      <td>0.602929</td>\n",
       "      <td>0.618977</td>\n",
       "      <td>0.598091</td>\n",
       "      <td>0.602929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>0.934036</td>\n",
       "      <td>0.602929</td>\n",
       "      <td>0.615681</td>\n",
       "      <td>0.599098</td>\n",
       "      <td>0.602929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.940291</td>\n",
       "      <td>0.604557</td>\n",
       "      <td>0.612662</td>\n",
       "      <td>0.600888</td>\n",
       "      <td>0.604557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>1.081934</td>\n",
       "      <td>0.532140</td>\n",
       "      <td>0.574767</td>\n",
       "      <td>0.511763</td>\n",
       "      <td>0.532140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.875700</td>\n",
       "      <td>0.958835</td>\n",
       "      <td>0.578519</td>\n",
       "      <td>0.605383</td>\n",
       "      <td>0.572746</td>\n",
       "      <td>0.578519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>1.023036</td>\n",
       "      <td>0.550854</td>\n",
       "      <td>0.574479</td>\n",
       "      <td>0.545134</td>\n",
       "      <td>0.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.866800</td>\n",
       "      <td>0.993217</td>\n",
       "      <td>0.570382</td>\n",
       "      <td>0.605076</td>\n",
       "      <td>0.564539</td>\n",
       "      <td>0.570382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.874300</td>\n",
       "      <td>0.981802</td>\n",
       "      <td>0.580960</td>\n",
       "      <td>0.587468</td>\n",
       "      <td>0.574340</td>\n",
       "      <td>0.580960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>1.007627</td>\n",
       "      <td>0.563059</td>\n",
       "      <td>0.592363</td>\n",
       "      <td>0.557405</td>\n",
       "      <td>0.563059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.888700</td>\n",
       "      <td>0.924650</td>\n",
       "      <td>0.606184</td>\n",
       "      <td>0.614859</td>\n",
       "      <td>0.600803</td>\n",
       "      <td>0.606184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.852700</td>\n",
       "      <td>0.933601</td>\n",
       "      <td>0.596420</td>\n",
       "      <td>0.607385</td>\n",
       "      <td>0.591007</td>\n",
       "      <td>0.596420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.885400</td>\n",
       "      <td>0.929795</td>\n",
       "      <td>0.605370</td>\n",
       "      <td>0.631004</td>\n",
       "      <td>0.600462</td>\n",
       "      <td>0.605370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>0.985170</td>\n",
       "      <td>0.572010</td>\n",
       "      <td>0.596363</td>\n",
       "      <td>0.565859</td>\n",
       "      <td>0.572010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.868600</td>\n",
       "      <td>1.008819</td>\n",
       "      <td>0.562246</td>\n",
       "      <td>0.588349</td>\n",
       "      <td>0.556149</td>\n",
       "      <td>0.562246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.934642</td>\n",
       "      <td>0.592352</td>\n",
       "      <td>0.614985</td>\n",
       "      <td>0.587587</td>\n",
       "      <td>0.592352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.854200</td>\n",
       "      <td>0.961077</td>\n",
       "      <td>0.594793</td>\n",
       "      <td>0.636167</td>\n",
       "      <td>0.584072</td>\n",
       "      <td>0.594793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.953835</td>\n",
       "      <td>0.580146</td>\n",
       "      <td>0.601924</td>\n",
       "      <td>0.573163</td>\n",
       "      <td>0.580146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.959752</td>\n",
       "      <td>0.580960</td>\n",
       "      <td>0.604859</td>\n",
       "      <td>0.575224</td>\n",
       "      <td>0.580960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.846300</td>\n",
       "      <td>0.958820</td>\n",
       "      <td>0.588283</td>\n",
       "      <td>0.609559</td>\n",
       "      <td>0.583750</td>\n",
       "      <td>0.588283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.809800</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0.581774</td>\n",
       "      <td>0.604696</td>\n",
       "      <td>0.577170</td>\n",
       "      <td>0.581774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157310/987570464.py:30: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric_f1 = load_metric(\"f1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=0.9459214385350545, metrics={'train_runtime': 3544.152, 'train_samples_per_second': 52.015, 'train_steps_per_second': 0.339, 'total_flos': 0.0, 'train_loss': 0.9459214385350545, 'epoch': 50.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39fb8002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9633054137229919,\n",
       " 'eval_wa': 0.5817737998372661,\n",
       " 'eval_ua': 0.6046955133998468,\n",
       " 'eval_f1': 0.5771699566879718,\n",
       " 'eval_accuracy': 0.5817737998372661,\n",
       " 'eval_runtime': 18.1726,\n",
       " 'eval_samples_per_second': 67.629,\n",
       " 'eval_steps_per_second': 0.88,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(output_dir)\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06e07928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-1.3889208 ,  1.845187  ,  0.15139765, -0.5911195 ],\n",
       "       [-0.06119258,  3.498305  ,  1.12946   , -4.8018875 ],\n",
       "       [ 0.1447005 ,  1.6797798 ,  1.5926493 , -3.902659  ],\n",
       "       ...,\n",
       "       [ 0.4033601 ,  4.1252613 , -1.1226912 , -3.063744  ],\n",
       "       [ 0.9112445 ,  4.0750275 , -0.7041302 , -4.220233  ],\n",
       "       [ 2.0044858 ,  2.3246527 , -0.6490066 , -4.012108  ]],\n",
       "      dtype=float32), label_ids=array([2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 0., 3.,\n",
       "       0., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 3., 3., 2., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 2., 2., 0., 2., 0., 3., 2.,\n",
       "       2., 2., 1., 3., 3., 1., 0., 2., 2., 2., 2., 3., 2., 2., 3., 3., 2.,\n",
       "       2., 2., 2., 2., 0., 2., 1., 2., 1., 2., 1., 1., 1., 1., 2., 1., 1.,\n",
       "       2., 1., 2., 1., 2., 1., 1., 2., 1., 2., 1., 2., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 2., 1., 2., 1., 2., 1., 2., 1., 1., 1., 1., 1., 0., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 1., 2., 2., 1., 2., 1.,\n",
       "       2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 2., 2., 2., 2., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 1., 2., 2., 2.,\n",
       "       1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 1., 1., 1., 1., 2., 1., 2., 1., 2., 2., 1., 1., 2., 2.,\n",
       "       2., 3., 2., 2., 3., 2., 2., 1., 1., 1., 1., 3., 2., 1., 3., 3., 3.,\n",
       "       3., 3., 1., 1., 2., 2., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1.,\n",
       "       1., 3., 1., 2., 3., 3., 3., 2., 3., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 2., 0., 0., 0., 2., 0., 0., 0., 0., 0., 2., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 2., 3., 2., 2., 2., 2., 3., 3., 3., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 2., 2., 2., 1., 1.,\n",
       "       2., 1., 2., 1., 2., 1., 1., 1., 2., 2., 1., 1., 1., 1., 2., 1., 1.,\n",
       "       2., 2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.], dtype=float32), metrics={'test_loss': 0.990439236164093, 'test_wa': 0.5951219512195122, 'test_ua': 0.5936311911818921, 'test_f1': 0.5898556384038381, 'test_accuracy': 0.5951219512195122, 'test_runtime': 10.0286, 'test_samples_per_second': 61.325, 'test_steps_per_second': 0.798})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c868b-5bff-4e4a-b9d0-850cfd0d2217",
   "metadata": {},
   "source": [
    "### Save projector and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f270c03-e7a5-4586-98c3-ac9f82ddbc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREEZE_PROJ_PATH = \"/home/freeze/{}/projector\".format(SESS_ID)\n",
    "FREEZE_CLAS_PATH = \"/home/freeze/{}/classifier\".format(SESS_ID)\n",
    "os.makedirs(FREEZE_PROJ_PATH, exist_ok=True)\n",
    "os.makedirs(FREEZE_CLAS_PATH, exist_ok=True)\n",
    "\n",
    "FREEZE_PROJ = os.path.join(FREEZE_PROJ_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "FREEZE_CLAS = os.path.join(FREEZE_CLAS_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "\n",
    "torch.save(model.projector.state_dict(), FREEZE_PROJ)\n",
    "torch.save(model.classifier.state_dict(), FREEZE_CLAS)\n",
    "\n",
    "model.projector.load_state_dict(torch.load(FREEZE_PROJ))\n",
    "model.classifier.load_state_dict(torch.load(FREEZE_CLAS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc244cf5-d891-4195-b507-ce7bc55c6fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
