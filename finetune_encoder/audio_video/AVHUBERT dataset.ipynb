{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d72d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import librosa\n",
    "\n",
    "import torch\n",
    "# from transformers import AutoFeatureExtractor, AutoProcessor, WhisperForConditionalGeneration\n",
    "# from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "from python_speech_features import logfbank\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b659dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\university\\FYT\\repos\\multi_modal_ser\\finetune_encoder\\audio_video\\av_hubert\\avhubert\n"
     ]
    }
   ],
   "source": [
    "%cd E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/fairseq\")\n",
    "import utils as avhubert_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6f2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "AUDIORATE = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bb9f0",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797f7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "class MMSERDataset(Dataset):\n",
    "    \"\"\"multi model ser dataset.\"\"\"\n",
    "    \n",
    "    def stacker(self, feats, stack_order):\n",
    "        \"\"\"\n",
    "        Concatenating consecutive audio frames\n",
    "        Args:\n",
    "        feats - numpy.ndarray of shape [T, F]\n",
    "        stack_order - int (number of neighboring frames to concatenate\n",
    "        Returns:\n",
    "        feats - numpy.ndarray of shape [T', F']\n",
    "        \"\"\"\n",
    "        feat_dim = feats.shape[1]\n",
    "        if len(feats) % stack_order != 0:\n",
    "            res = stack_order - len(feats) % stack_order\n",
    "            res = np.zeros([res, feat_dim]).astype(feats.dtype)\n",
    "            feats = np.concatenate([feats, res], axis=0)\n",
    "        feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)\n",
    "        return feats\n",
    "        \n",
    "    def __load_label__(self, cutmap_path):\n",
    "        sheet_df = pd.DataFrame()\n",
    "        for ses in range(1, 6):\n",
    "            extractionmapPATH = cutmap_path + \\\n",
    "                str(ses)+'.xlsx'\n",
    "            xl = pd.ExcelFile(extractionmapPATH)\n",
    "            sheets = xl.sheet_names\n",
    "            for sheet in sheets:\n",
    "                sheet_df = pd.concat([sheet_df, xl.parse(sheet)])\n",
    "        self.df_ = sheet_df\n",
    "        \n",
    "        # remove samples not agreed\n",
    "        self.df_ = pd.merge(self.df_, self.df_text, on=[\"smp_id\"])\n",
    "        self.df_[\"emotion_id\"] = self.df_[\"emotion\"].map(self.emo2id)\n",
    "        self.df_ = self.df_[self.df_[\"emotion_id\"].notna()].reset_index(drop=True)\n",
    "        self.df_[\"session\"] = self.df_[\"smp_id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "        self.df_ = self.df_[self.df_[\"smp_id\"].str.startswith(\"Ses01F_impro\")].reset_index(drop=True)\n",
    "        \n",
    "    def __load_text__(self, text_path):\n",
    "        self.df_text = pd.read_csv(text_path)\n",
    "        pass\n",
    "    \n",
    "    def __load_audio__(self, fn_path):\n",
    "        self.fn_list = list(self.df_[\"smp_id\"])\n",
    "        self.raw_list = []\n",
    "        for fn in self.fn_list:\n",
    "            self.raw_list.append(wavfile.read(os.path.join(fn_path, fn)+'.wav')[1])\n",
    "    \n",
    "    def __load_video__(self, idx):\n",
    "        frames = avhubert_utils.load_video(os.path.join(self.video_path, idx+\".mp4\"))\n",
    "#         transform = avhubert_utils.Compose([\n",
    "#           avhubert_utils.Normalize(0.0, 255.0),\n",
    "#           avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
    "#           avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
    "        # frames = transform(frames)\n",
    "        frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        video_feats = frames\n",
    "        return video_feats\n",
    "    \n",
    "    def __init__(self, \n",
    "                 fn_path, \n",
    "                 cutmap_path, \n",
    "                 text_path, \n",
    "                 video_path, \n",
    "                 emo2id,\n",
    "                 audio_in_features = 104):\n",
    "        \n",
    "        self.emo2id = emo2id\n",
    "        self.audio_in_features = audio_in_features\n",
    "        self.video_path = video_path\n",
    "        self.__load_text__(text_path)\n",
    "        self.__load_label__(cutmap_path)\n",
    "        self.__load_audio__(fn_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df_.shape[0]\n",
    "    \n",
    "    def __getsingle__(self, idx):\n",
    "        raw_audio = self.raw_list[idx]\n",
    "        video_feats = self.__load_video__(self.fn_list[idx])\n",
    "        audio_feats = logfbank(raw_audio, samplerate=AUDIORATE).astype(np.float32) # [T, F]\n",
    "        audio_feats = self.stacker(audio_feats, self.audio_in_features//26) # [T/stack_order_audio, F*stack_order_audio]\n",
    "\n",
    "        diff = audio_feats.shape[0] - video_feats.shape[2]\n",
    "        if diff < 0:\n",
    "            audio_feats = np.concatenate([audio_feats, np.zeros([-diff, audio_feats.shape[-1]], dtype=audio_feats.dtype)])\n",
    "        elif diff > 0:\n",
    "            audio_feats = audio_feats[:-diff]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_feats = torch.from_numpy(audio_feats.astype(np.float32)).T\n",
    "            audio_feats = F.layer_norm(audio_feats, audio_feats.shape[1:])\n",
    "            audio_feats = audio_feats.unsqueeze(dim=0)\n",
    "        return audio_feats, video_feats\n",
    "    \n",
    "    def collate(self, audio, video, max_size=500):\n",
    "        padded_audio = pad_sequence([a.T.squeeze() for a in audio], batch_first=True)\n",
    "        padded_video = pad_sequence([v.squeeze() for v in video], batch_first=True)[:, np.newaxis, : ,:,:]\n",
    "        mask = torch.zeros_like(padded_audio)\n",
    "        mask[padded_audio != 0] = 1\n",
    "        return padded_audio, padded_video, mask\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            ret_dict = {}\n",
    "            for key in self.__getitem__(0).keys():\n",
    "                ret_dict[key] = [self.__getitem__(i)[key] for i in range(*idx.indices(len(self)))]\n",
    "            \n",
    "            padded_audio, padded_video, padding_mask = self.collate(ret_dict[\"audio\"], ret_dict[\"video\"])\n",
    "            ret_dict[\"padding_mask\"] = padding_mask\n",
    "            ret_dict[\"audio\"] = padded_audio.transpose(1, 2)\n",
    "            ret_dict[\"video\"] = padded_video\n",
    "            return ret_dict\n",
    "        else:\n",
    "            audio_feats, video_feats = self.__getsingle__(idx)\n",
    "\n",
    "            return {\n",
    "                \"sess\": list(self.df_[\"session\"])[idx],\n",
    "                \"fn\": self.fn_list[idx],\n",
    "                \"audio\": audio_feats,\n",
    "                \"video\": video_feats,\n",
    "                \"text\": list(self.df_[\"transcript\"])[idx],\n",
    "                \"labels\": list(self.df_[\"emotion_id\"])[idx]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7bdd0",
   "metadata": {},
   "source": [
    "### Dataset Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c72c09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo2id_dict={\n",
    "    \"hap\": 0,\n",
    "    \"ang\": 1,\n",
    "    \"neu\": 2,\n",
    "    \"sad\": 3,\n",
    "    \"exc\": 0,\n",
    "       }\n",
    "\n",
    "mmser_ds = MMSERDataset(fn_path = \"E:/datasets/preprocessed/spectrogram/raw\", \n",
    "                        cutmap_path = 'E:/datasets/preprocessed/extractionmap/cut_extractionmap', \n",
    "                        text_path = \"E:/datasets/preprocessed/transcipt/transcript.csv\", \n",
    "                        video_path = \"E:/datasets/preprocessed/face_raw\",\n",
    "                        emo2id=emo2id_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3deb8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mmser_ds[22:24])\n",
    "# print(len(mmser_ds))\n",
    "# print(mmser_ds.df_[\"emotion_id\"].value_counts())\n",
    "# mmser_ds.df_[\"emotion_id\"].value_counts().plot(kind=\"pie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfc66e",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e21941aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mmser_ds, \"E:/datasets/preprocessed/dataset/avhubert_ds.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ed2141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>iframe</th>\n",
       "      <th>fframe</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker</th>\n",
       "      <th>smp_id</th>\n",
       "      <th>emovectorA</th>\n",
       "      <th>emovectorB</th>\n",
       "      <th>emovectorC</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>transcript</th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100642</td>\n",
       "      <td>131771</td>\n",
       "      <td>neu</td>\n",
       "      <td>L</td>\n",
       "      <td>Ses01F_impro01_F000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>Excuse me.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ses01F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>160161</td>\n",
       "      <td>182280</td>\n",
       "      <td>neu</td>\n",
       "      <td>L</td>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ses01F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>238196</td>\n",
       "      <td>288280</td>\n",
       "      <td>neu</td>\n",
       "      <td>L</td>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4</td>\n",
       "      <td>Is there a problem?</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ses01F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>439361</td>\n",
       "      <td>503840</td>\n",
       "      <td>neu</td>\n",
       "      <td>L</td>\n",
       "      <td>Ses01F_impro01_F005</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Well what's the problem?  Let me change it.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ses01F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>1364321</td>\n",
       "      <td>1408320</td>\n",
       "      <td>ang</td>\n",
       "      <td>L</td>\n",
       "      <td>Ses01F_impro01_F012</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>23</td>\n",
       "      <td>That's out of control.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ses01F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x   iframe   fframe emotion speaker               smp_id  \\\n",
       "0             0   100642   131771     neu       L  Ses01F_impro01_F000   \n",
       "1             2   160161   182280     neu       L  Ses01F_impro01_F001   \n",
       "2             4   238196   288280     neu       L  Ses01F_impro01_F002   \n",
       "3             9   439361   503840     neu       L  Ses01F_impro01_F005   \n",
       "4            23  1364321  1408320     ang       L  Ses01F_impro01_F012   \n",
       "\n",
       "   emovectorA  emovectorB  emovectorC  Unnamed: 0_y  \\\n",
       "0         2.5         2.5         2.5             0   \n",
       "1         2.5         2.5         2.5             2   \n",
       "2         2.5         2.5         2.5             4   \n",
       "3         2.5         3.5         2.0             9   \n",
       "4         2.0         3.5         3.5            23   \n",
       "\n",
       "                                    transcript  emotion_id session  \n",
       "0                                   Excuse me.         2.0  Ses01F  \n",
       "1                                        Yeah.         2.0  Ses01F  \n",
       "2                          Is there a problem?         2.0  Ses01F  \n",
       "3  Well what's the problem?  Let me change it.         2.0  Ses01F  \n",
       "4                       That's out of control.         1.0  Ses01F  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmser_ds[2]\n",
    "mmser_ds.df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0fe2f",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dbff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a8f6c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\university\\FYT\\repos\\multi_modal_ser\\finetune_encoder\\audio_video\\av_hubert\\avhubert\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Could not infer task type from {'_name': 'av_hubert_pretraining', 'is_s2s': True, 'data': '/checkpoint/bshi/data/lrs3//exp/ls-hubert/tune-modality/all_tsv/', 'label_dir': '/checkpoint/bshi/data/lrs3//exp/ls-hubert/tune-modality/all_bpe/unigram1000/', 'normalize': True, 'labels': ['wrd'], 'single_target': True, 'stack_order_audio': 4, 'tokenizer_bpe_name': 'sentencepiece', 'max_sample_size': 500, 'modalities': ['video'], 'image_aug': True, 'pad_audio': True, 'random_crop': False, 'tokenizer_bpe_model': '/checkpoint/bshi/data/lrs3//lang/spm/spm_unigram1000.model', 'fine_tuning': True}. Available tasks: dict_keys(['audio_pretraining', 'cross_lingual_lm', 'denoising', 'hubert_pretraining', 'language_modeling', 'legacy_masked_lm', 'masked_lm', 'multilingual_denoising', 'multilingual_masked_lm', 'translation', 'multilingual_translation', 'online_backtranslation', 'semisupervised_translation', 'sentence_prediction', 'sentence_ranking', 'speech_to_text', 'simul_speech_to_text', 'simul_text_to_text', 'translation_from_pretrained_bart', 'translation_from_pretrained_xlm', 'translation_lev', 'translation_multi_simple_epoch', 'dummy_lm', 'dummy_masked_lm', 'dummy_mt'])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16036\\3001115891.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheckpoint_utils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"E:/check_pts/avhubert.pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaved_cfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_ensemble_and_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'decoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/fairseq\\fairseq\\checkpoint_utils.py\u001b[0m in \u001b[0;36mload_model_ensemble_and_task\u001b[1;34m(filenames, arg_overrides, task, strict, suffix, num_shards, state)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                 \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"task_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/fairseq\\fairseq\\tasks\\__init__.py\u001b[0m in \u001b[0;36msetup_task\u001b[1;34m(cfg, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     assert (\n\u001b[0;32m     43\u001b[0m         \u001b[0mtask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     ), f\"Could not infer task type from {cfg}. Available tasks: {TASK_REGISTRY.keys()}\"\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Could not infer task type from {'_name': 'av_hubert_pretraining', 'is_s2s': True, 'data': '/checkpoint/bshi/data/lrs3//exp/ls-hubert/tune-modality/all_tsv/', 'label_dir': '/checkpoint/bshi/data/lrs3//exp/ls-hubert/tune-modality/all_bpe/unigram1000/', 'normalize': True, 'labels': ['wrd'], 'single_target': True, 'stack_order_audio': 4, 'tokenizer_bpe_name': 'sentencepiece', 'max_sample_size': 500, 'modalities': ['video'], 'image_aug': True, 'pad_audio': True, 'random_crop': False, 'tokenizer_bpe_model': '/checkpoint/bshi/data/lrs3//lang/spm/spm_unigram1000.model', 'fine_tuning': True}. Available tasks: dict_keys(['audio_pretraining', 'cross_lingual_lm', 'denoising', 'hubert_pretraining', 'language_modeling', 'legacy_masked_lm', 'masked_lm', 'multilingual_denoising', 'multilingual_masked_lm', 'translation', 'multilingual_translation', 'online_backtranslation', 'semisupervised_translation', 'sentence_prediction', 'sentence_ranking', 'speech_to_text', 'simul_speech_to_text', 'simul_text_to_text', 'translation_from_pretrained_bart', 'translation_from_pretrained_xlm', 'translation_lev', 'translation_multi_simple_epoch', 'dummy_lm', 'dummy_masked_lm', 'dummy_mt'])"
     ]
    }
   ],
   "source": [
    "%cd E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\n",
    "\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "ckpt_path = \"E:/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc78f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
