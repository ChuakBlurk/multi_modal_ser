{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Prepare Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6c4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/av_hubert.git\n",
    "\n",
    "# %cd av_hubert\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# !pip install scipy\n",
    "# !pip install sentencepiece\n",
    "# !pip install python_speech_features\n",
    "# !pip install scikit-video\n",
    "\n",
    "# %cd fairseq\n",
    "# !python -m pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e231ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\university\\FYT\\repos\\multi_modal_ser\\finetune_encoder\\audio_video\\av_hubert\\avhubert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1 (default, Oct 28 2018, 08:39:03) [MSC v.1912 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "%cd E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/fairseq\")\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "import cv2\n",
    "import tempfile\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import utils as avhubert_utils\n",
    "from argparse import Namespace\n",
    "from IPython.display import HTML\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import logfbank\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7624284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacker(feats, stack_order):\n",
    "    \"\"\"\n",
    "    Concatenating consecutive audio frames\n",
    "    Args:\n",
    "    feats - numpy.ndarray of shape [T, F]\n",
    "    stack_order - int (number of neighboring frames to concatenate\n",
    "    Returns:\n",
    "    feats - numpy.ndarray of shape [T', F']\n",
    "    \"\"\"\n",
    "    feat_dim = feats.shape[1]\n",
    "    if len(feats) % stack_order != 0:\n",
    "        res = stack_order - len(feats) % stack_order\n",
    "        res = np.zeros([res, feat_dim]).astype(feats.dtype)\n",
    "        feats = np.concatenate([feats, res], axis=0)\n",
    "    feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1442ff",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15addb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt -O E:/check_pts/avhubert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edf8e1",
   "metadata": {},
   "source": [
    "### Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da14d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: fine-tuned\n"
     ]
    }
   ],
   "source": [
    "user_dir = \"E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"E:/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2d7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 104)\n",
      "torch.Size([1, 1, 192, 88, 88])\n"
     ]
    }
   ],
   "source": [
    "smp_id = \"Ses01F_impro05_M017\"\n",
    "video_path = \"E:/datasets/preprocessed/face_raw/{}/{}.mp4\".format(smp_id.split(\"_\")[0][:-1],smp_id)\n",
    "audio_path = \"E:/datasets/preprocessed/spectrogram/raw/{}.wav\".format(smp_id)\n",
    "\n",
    "# Load Video\n",
    "# Mute transform first\n",
    "transform = avhubert_utils.Compose([\n",
    "  avhubert_utils.Normalize(0.0, 255.0),\n",
    "  avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
    "  avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
    "\n",
    "frames = avhubert_utils.load_video(video_path)\n",
    "# frames = transform(frames)\n",
    "frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "video_feats = frames\n",
    "\n",
    "# Load Audio\n",
    "sample_rate, wav_data = wavfile.read(audio_path)\n",
    "in_features = model.feature_extractor_audio.proj.in_features\n",
    "assert sample_rate == 16_000 and len(wav_data.shape) == 1\n",
    "audio_feats = logfbank(wav_data, samplerate=sample_rate).astype(np.float32) # [T, F]\n",
    "audio_feats = stacker(audio_feats, in_features//26) # [T/stack_order_audio, F*stack_order_audio]\n",
    "\n",
    "# Match Audio Video\n",
    "print(audio_feats.shape)\n",
    "print(video_feats.shape)\n",
    "\n",
    "diff = audio_feats.shape[0] - video_feats.shape[2]\n",
    "if diff < 0:\n",
    "    audio_feats = np.concatenate([audio_feats, np.zeros([-diff, audio_feats.shape[-1]], dtype=audio_feats.dtype)])\n",
    "elif diff > 0:\n",
    "    audio_feats = audio_feats[:-diff]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "audio_feats = torch.from_numpy(audio_feats.astype(np.float32)).T\n",
    "audio_feats = F.layer_norm(audio_feats, audio_feats.shape[1:])\n",
    "audio_feats = audio_feats.unsqueeze(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa7321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video feature shape: torch.Size([192, 768])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Specify output_layer if you want to extract feature of an intermediate layer\n",
    "    feature, _ = model.extract_finetune(source={'video': video_feats , \n",
    "                                                'audio': audio_feats}, padding_mask=None, output_layer=None)\n",
    "    feature = feature.squeeze(dim=0) \n",
    "print(f\"Video feature shape: {feature.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c90a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Model Standalone Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4e65941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "AUDIORATE=16000\n",
    "class MMSERDataset(Dataset):\n",
    "    \"\"\"multi model ser dataset.\"\"\"\n",
    "    \n",
    "    def stacker(self, feats, stack_order):\n",
    "        \"\"\"\n",
    "        Concatenating consecutive audio frames\n",
    "        Args:\n",
    "        feats - numpy.ndarray of shape [T, F]\n",
    "        stack_order - int (number of neighboring frames to concatenate\n",
    "        Returns:\n",
    "        feats - numpy.ndarray of shape [T', F']\n",
    "        \"\"\"\n",
    "        feat_dim = feats.shape[1]\n",
    "        if len(feats) % stack_order != 0:\n",
    "            res = stack_order - len(feats) % stack_order\n",
    "            res = np.zeros([res, feat_dim]).astype(feats.dtype)\n",
    "            feats = np.concatenate([feats, res], axis=0)\n",
    "        feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)\n",
    "        return feats\n",
    "        \n",
    "    def __load_label__(self, cutmap_path):\n",
    "        sheet_df = pd.DataFrame()\n",
    "        for ses in range(1, 6):\n",
    "            extractionmapPATH = cutmap_path + \\\n",
    "                str(ses)+'.xlsx'\n",
    "            xl = pd.ExcelFile(extractionmapPATH)\n",
    "            sheets = xl.sheet_names\n",
    "            for sheet in sheets:\n",
    "                sheet_df = pd.concat([sheet_df, xl.parse(sheet)])\n",
    "        self.df_ = sheet_df\n",
    "        \n",
    "        # remove samples not agreed\n",
    "        self.df_ = pd.merge(self.df_, self.df_text, on=[\"smp_id\"])\n",
    "        self.df_[\"emotion_id\"] = self.df_[\"emotion\"].map(self.emo2id)\n",
    "        self.df_ = self.df_[self.df_[\"emotion_id\"].notna()].reset_index(drop=True)\n",
    "        self.df_[\"session\"] = self.df_[\"smp_id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "        self.df_ = self.df_[self.df_[\"smp_id\"].str.startswith(\"Ses01F_impro\")].reset_index(drop=True)\n",
    "        \n",
    "    def __load_text__(self, text_path):\n",
    "        self.df_text = pd.read_csv(text_path)\n",
    "        pass\n",
    "    \n",
    "    def __load_audio__(self, fn_path):\n",
    "        self.fn_list = list(self.df_[\"smp_id\"])\n",
    "        self.raw_list = []\n",
    "        for fn in self.fn_list:\n",
    "            self.raw_list.append(wavfile.read(os.path.join(fn_path, fn)+'.wav')[1])\n",
    "    \n",
    "    def __load_video__(self, idx):\n",
    "        frames = avhubert_utils.load_video(os.path.join(self.video_path, idx.split(\"_\")[0][:-1], idx+\".mp4\"))\n",
    "#         transform = avhubert_utils.Compose([\n",
    "#           avhubert_utils.Normalize(0.0, 255.0),\n",
    "#           avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
    "#           avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
    "        # frames = transform(frames)\n",
    "        frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        video_feats = frames\n",
    "        return video_feats\n",
    "    \n",
    "    def __init__(self, \n",
    "                 fn_path, \n",
    "                 cutmap_path, \n",
    "                 text_path, \n",
    "                 video_path, \n",
    "                 emo2id,\n",
    "                 audio_in_features = 104):\n",
    "        \n",
    "        self.emo2id = emo2id\n",
    "        self.audio_in_features = audio_in_features\n",
    "        self.video_path = video_path\n",
    "        self.__load_text__(text_path)\n",
    "        self.__load_label__(cutmap_path)\n",
    "        self.__load_audio__(fn_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df_.shape[0]\n",
    "    \n",
    "    def __getsingle__(self, idx):\n",
    "        raw_audio = self.raw_list[idx]\n",
    "        video_feats = self.__load_video__(self.fn_list[idx])\n",
    "        audio_feats = logfbank(raw_audio, samplerate=AUDIORATE).astype(np.float32) # [T, F]\n",
    "        audio_feats = self.stacker(audio_feats, self.audio_in_features//26) # [T/stack_order_audio, F*stack_order_audio]\n",
    "\n",
    "        diff = audio_feats.shape[0] - video_feats.shape[2]\n",
    "        if diff < 0:\n",
    "            audio_feats = np.concatenate([audio_feats, np.zeros([-diff, audio_feats.shape[-1]], dtype=audio_feats.dtype)])\n",
    "        elif diff > 0:\n",
    "            audio_feats = audio_feats[:-diff]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_feats = torch.from_numpy(audio_feats.astype(np.float32)).T\n",
    "            audio_feats = F.layer_norm(audio_feats, audio_feats.shape[1:])\n",
    "            audio_feats = audio_feats.unsqueeze(dim=0)\n",
    "        return audio_feats, video_feats\n",
    "    \n",
    "    def collate(self, audio, video, max_size=500):\n",
    "        padded_audio = pad_sequence([a.T.squeeze() for a in audio]+[torch.empty(500, 104)], batch_first=True)[:-1]\n",
    "#         print(padded_audio)\n",
    "        padded_video = pad_sequence([v.squeeze() for v in video]+[torch.empty(500,88,88)], batch_first=True)[:-1, np.newaxis, : ,:,:]\n",
    "        mask = torch.zeros_like(padded_audio)\n",
    "        mask[padded_audio != 0] = 1\n",
    "        return padded_audio, padded_video, mask\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            ret_dict = {}\n",
    "            for key in self.__getitem__(0).keys():\n",
    "                ret_dict[key] = [self.__getitem__(i)[key] for i in range(*idx.indices(len(self)))]\n",
    "            \n",
    "#             padded_audio, padded_video, padding_mask = self.collate(ret_dict[\"audio\"], ret_dict[\"video\"])\n",
    "#             ret_dict[\"padding_mask\"] = padding_mask\n",
    "#             ret_dict[\"audio\"] = padded_audio.transpose(1, 2)\n",
    "#             ret_dict[\"video\"] = padded_video\n",
    "\n",
    "            ret_dict[\"audio\"] = torch.stack(ret_dict[\"audio\"])\n",
    "            ret_dict[\"video\"] = torch.stack(ret_dict[\"video\"])\n",
    "            ret_dict[\"padding_mask\"] = torch.stack(ret_dict[\"padding_mask\"])\n",
    "            return ret_dict\n",
    "        \n",
    "        else:\n",
    "            audio_feats, video_feats = self.__getsingle__(idx)\n",
    "            padded_audio, padded_video, padding_mask = self.collate([audio_feats], [video_feats])\n",
    "            \n",
    "            return {\n",
    "                \"padding_mask\": padding_mask[0][:500, :],\n",
    "                \"sess\": list(self.df_[\"session\"])[idx],\n",
    "                \"fn\": self.fn_list[idx],\n",
    "                \"audio\": padded_audio[0][:500, :].transpose(0,1),\n",
    "                \"video\": padded_video[0][:, :500, :, :],\n",
    "                \"text\": list(self.df_[\"transcript\"])[idx],\n",
    "                \"labels\": list(self.df_[\"emotion_id\"])[idx]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e339c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmser_ds = torch.load(\"E:/datasets/preprocessed/dataset/avhubert_ds.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7171d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 104, 500])\n",
      "torch.Size([2, 1, 500, 88, 88])\n",
      "torch.Size([2, 500, 104])\n"
     ]
    }
   ],
   "source": [
    "print(mmser_ds[2:4]['audio'].shape)\n",
    "print(mmser_ds[2:4]['video'].shape)\n",
    "print(mmser_ds[2:4]['padding_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3fd6261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104, 500])\n",
      "torch.Size([1, 500, 88, 88])\n",
      "torch.Size([500, 104])\n"
     ]
    }
   ],
   "source": [
    "print(mmser_ds[24]['audio'].shape)\n",
    "print(mmser_ds[24]['video'].shape)\n",
    "print(mmser_ds[24]['padding_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e62f9089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.6003e-01,  3.3108e-02, -4.2043e-02,  ...,  1.5355e-02,\n",
       "            1.7279e-01, -1.2281e-01],\n",
       "          [ 3.6391e-02,  1.7289e-02, -7.5044e-02,  ..., -3.2006e-01,\n",
       "            3.2333e-01,  7.7511e-02],\n",
       "          [ 1.0995e-01,  2.0536e-04, -1.0733e-01,  ..., -3.8327e-01,\n",
       "            2.9839e-01,  1.5985e-01],\n",
       "          ...,\n",
       "          [-2.3690e-02,  1.7321e-01,  1.5092e-03,  ..., -2.9121e-01,\n",
       "            1.0547e-01,  1.7090e-01],\n",
       "          [-1.1662e-01,  1.7394e-01,  1.2207e-01,  ..., -2.5699e-01,\n",
       "            1.7086e-01,  1.1992e-01],\n",
       "          [-2.5376e-01,  1.5554e-01,  2.1772e-01,  ..., -2.1937e-01,\n",
       "            2.1669e-01,  5.0649e-02]],\n",
       " \n",
       "         [[-2.6078e-01,  1.9918e-02, -4.4009e-02,  ...,  1.8448e-02,\n",
       "            1.6106e-01, -1.1463e-01],\n",
       "          [ 4.5193e-02, -1.0702e-02, -6.7196e-02,  ..., -3.3309e-01,\n",
       "            3.1324e-01,  9.3462e-02],\n",
       "          [ 1.1324e-01, -8.6834e-04, -8.2459e-02,  ..., -3.8185e-01,\n",
       "            2.9010e-01,  1.6326e-01],\n",
       "          ...,\n",
       "          [-1.8550e-02,  1.7241e-01,  4.6922e-03,  ..., -2.8427e-01,\n",
       "            8.7463e-02,  1.5649e-01],\n",
       "          [-1.0543e-01,  1.7146e-01,  1.2625e-01,  ..., -2.5143e-01,\n",
       "            1.6129e-01,  1.1228e-01],\n",
       "          [-2.4693e-01,  1.5749e-01,  2.3075e-01,  ..., -2.1023e-01,\n",
       "            2.1441e-01,  4.3838e-02]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "AUDIORATE = 16000\n",
    "\n",
    "outputs = model.extract_finetune(mmser_ds[:2])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4fb3b7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 768])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b934a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = mmser_ds[:4][\"padding_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3fabec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_padding_mask(\n",
    "        features: torch.Tensor, padding_mask: torch.Tensor,\n",
    "    ):\n",
    "        extra = padding_mask.size(1) % features.size(1)\n",
    "        if extra > 0:\n",
    "            padding_mask = padding_mask[:, :-extra]\n",
    "        padding_mask = padding_mask.view(\n",
    "            padding_mask.size(0), features.size(1), -1\n",
    "        )\n",
    "        padding_mask = padding_mask.all(-1)\n",
    "        return padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3b0e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "forwarded_attention_mask = forward_padding_mask(outputs[0], attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb06072",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9da70288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVHUBERTClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, hidden_size, proj_size, num_labels=4):\n",
    "        super(AVHUBERTClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.num_labels = num_labels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.proj_size = proj_size\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.projector = nn.Linear(self.hidden_size, self.proj_size)\n",
    "        self.classifier = nn.Linear(self.proj_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        audio, video, padding_mask, **kwargs\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder.extract_finetune(\n",
    "                {\n",
    "                    \"audio\": audio, \n",
    "                    \"video\": video,\n",
    "                    \"padding_mask\": padding_mask\n",
    "                }\n",
    "            )\n",
    "            hidden_states = outputs[0]\n",
    "        \n",
    "        hidden_states = self.projector(hidden_states)\n",
    "        attention_mask = padding_mask\n",
    "        padding_mask = forward_padding_mask(hidden_states, attention_mask)\n",
    "        hidden_states[~padding_mask] = 0.0\n",
    "        pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return {\"logits\":logits}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "95d16866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-0.0002, -0.0439, -0.0396, -0.0240],\n",
       "         [ 0.0034, -0.0419, -0.0421, -0.0219],\n",
       "         [ 0.0348, -0.0315, -0.0330, -0.0251],\n",
       "         [ 0.0461, -0.0290, -0.0279, -0.0394]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = AVHUBERTClassifier(model, 768, 256, mmser_ds.df_[\"emotion_id\"].nunique())\n",
    "classifier(**mmser_ds[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eadf7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ds(sess_id):\n",
    "    train_size = int(len(sess_ds[sess_id+\"_train\"])*0.75)\n",
    "    val_size = len(sess_ds[sess_id+\"_train\"])-train_size\n",
    "    train_set, val_set = torch.utils.data.random_split(sess_ds[sess_id+\"_train\"], [train_size, val_size])\n",
    "    test_set = sess_ds[sess_id+\"_test\"]\n",
    "\n",
    "    print(\"Train Samples:\", len(train_set))\n",
    "    print(\"Val Samples:\", len(val_set))\n",
    "    print(\"Test Samples:\", len(test_set))\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ac2374d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 180\n",
      "Val Samples: 61\n",
      "Test Samples: 241\n"
     ]
    }
   ],
   "source": [
    "meta_df_ = mmser_ds.df_\n",
    "sess_dict = meta_df_.groupby(\"session\").groups\n",
    "all_indices = set(meta_df_.index.tolist())\n",
    "sess_ds = {}\n",
    "for sess in sess_dict:\n",
    "#     sess_ds[sess+\"_train\"] = Subset(mmser_ds, \n",
    "#                                     indices=list(all_indices-set(sess_dict[sess])))\n",
    "    sess_ds[sess+\"_test\"] = Subset(mmser_ds, \n",
    "                                    indices=sess_dict[sess])\n",
    "    sess_ds[sess+\"_train\"] = Subset(mmser_ds, \n",
    "                                    indices=sess_dict[sess])\n",
    "    \n",
    "    \n",
    "    \n",
    "SESS_ID = list(sess_dict.keys())[0]\n",
    "train_set, val_set, test_set = build_ds(SESS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b53bf38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").type(torch.LongTensor).to(device)\n",
    "        inputs = {\"video\": inputs[\"video\"], \"audio\":inputs[\"audio\"], \"padding_mask\":inputs[\"padding_mask\"]}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss() \n",
    "        loss = loss_fct(logits.view(-1, self.model.num_labels), labels.view(-1))        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "def weighted_acc(y_true, y_pred):\n",
    "    return np.sum((np.array(y_pred).ravel() == np.array(y_true).ravel()))*1.0/len(y_true)\n",
    "    \n",
    "def unweighted_acc(y_true, y_pred):\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    classes = np.unique(y_true)\n",
    "    classes_accuracies = np.zeros(classes.shape[0])\n",
    "    for num, cls in enumerate(classes):\n",
    "        classes_accuracies[num] = weighted_acc(y_true[y_true == cls], y_pred[y_true == cls])\n",
    "    return np.mean(classes_accuracies)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    metric_f1 = load_metric(\"f1\")\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_ = metric_f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    acc_ = metric_acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    \n",
    "    return {\"wa\":weighted_acc(labels, predictions), \n",
    "            \"ua\":unweighted_acc(labels, predictions),\n",
    "            \"f1\":f1_, \n",
    "            \"accuracy\":acc_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da5c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "63d6f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=2)\n",
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444e015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c843bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a40eb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=os.path.join(\"avhubert\")\n",
    "\n",
    "training_args = TrainingArguments(output_dir,report_to=\"wandb\")\n",
    "training_args.remove_unused_columns=False\n",
    "training_args.per_device_train_batch_size=4\n",
    "training_args.per_device_eval_batch_size=4\n",
    "training_args.logging_steps = 3 # int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.eval_steps = 3 #int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.evaluation_strategy=\"steps\" \n",
    "training_args.logging_strategy=\"steps\"\n",
    "training_args.load_best_model_at_end=True,\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.learning_rate=1e-3\n",
    "training_args.num_train_epochs=50\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ff67c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/2250 00:20 < 6:22:32, 0.10 it/s, Epoch 0.07/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/16 01:06 < 01:16, 0.10 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 500, 88, 88] at entry 0 and [1, 570, 88, 88] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15120\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1650\u001b[0m         )\n\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2018\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2020\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2021\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2319\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_metrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2321\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2322\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3058\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3059\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3060\u001b[1;33m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3061\u001b[0m         )\n\u001b[0;32m   3062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3233\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3234\u001b[0m         \u001b[1;31m# Main evaluation loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3235\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3236\u001b[0m             \u001b[1;31m# Update the observed num examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3237\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch_default_data_collator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf_default_data_collator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\py37\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label_ids\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 500, 88, 88] at entry 0 and [1, 570, 88, 88] at entry 2"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967ebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
