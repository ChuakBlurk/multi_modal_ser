{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Prepare Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485ef788-832b-4f9d-ad4d-b43c8ae38e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/ChuakBlurk/vidaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041460cc-7376-4d29-b95d-156e4b21b530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install omegaconf==2.1.1\n",
    "# !pip install hydra-core==1.1.1\n",
    "# !pip install -U numpy==1.23.5\n",
    "# !apt-get update && apt-get install -y python3-opencv\n",
    "# !pip install opencv-python\n",
    "# !pip install scikit-image \n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate -U\n",
    "# !pip install wandb\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6c4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/av_hubert.git\n",
    "\n",
    "# %cd av_hubert\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# !pip install scipy\n",
    "# !pip install sentencepiece\n",
    "# !pip install python_speech_features\n",
    "# !pip install scikit-video\n",
    "\n",
    "# %cd fairseq\n",
    "# !pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e231ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert\n",
      "3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "import cv2\n",
    "import tempfile\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "sys.path.append(\"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\")\n",
    "%cd /home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/\n",
    "import utils as avhubert_utils\n",
    "from argparse import Namespace\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128ecb29-946d-4fae-b806-17946ca63f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Sun Nov  5 09:55:08 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:C2:00.0 Off |                  Off |\n",
      "|  0%   40C    P8              25W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1442ff",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15addb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"/home/check_pts/\")\n",
    "# # !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt -O /home/check_pts/avhubert.pt\n",
    "# !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/clean-pretrain/base_vox_iter4.pt -O /home/check_pts/avhubert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edf8e1",
   "metadata": {},
   "source": [
    "### Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da14d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: pre-trained w/o fine-tuning\n"
     ]
    }
   ],
   "source": [
    "user_dir = \"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"/home/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7d434",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86df1ecf-343d-4464-824b-4adf070a8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e339c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_ds import AVHUBERTDataset\n",
    "mmser_ds = torch.load(\"/home/avhubert_ds2.pt\")\n",
    "mmser_ds.video_path = \"/home/face_raw/\"\n",
    "\n",
    "# outputs = model.extract_finetune(mmser_ds[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4043f023-01f6-4abc-aa9d-665b1ee7fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5531/5531 [14:29<00:00,  6.36it/s]\n"
     ]
    }
   ],
   "source": [
    "mmser_ds.cached = False\n",
    "mmser_ds.__cache__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb06072",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da70288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_classifier import AVHUBERTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d16866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = AVHUBERTClassifier(model, 768, 256, mmser_ds.df_[\"emotion_id\"].nunique())\n",
    "# classifier(**mmser_ds[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d940a",
   "metadata": {},
   "source": [
    "### Build Train Test DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c429bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_ = mmser_ds.df_\n",
    "mmser_ds.df_[\"bigsess\"] = mmser_ds.df_[\"session\"].apply(lambda x: x[:-1])\n",
    "sess_dict = mmser_ds.df_.groupby(\"bigsess\").groups\n",
    "all_indices = set(mmser_ds.df_.index.tolist())\n",
    "\n",
    "# sess_ds = {}\n",
    "# for i in range(1,6):\n",
    "#     sess = \"Ses0{}\".format(i)\n",
    "#     sess_val = \"Ses0{}\".format(i%5+1)\n",
    "#     sess_ds[sess+\"_test\"] = Subset(mmser_ds, \n",
    "#                                     indices=sess_dict[sess])\n",
    "#     # sess_ds[sess+\"_val\"] = Subset(mmser_ds, \n",
    "#     #                                 indices=sess_dict[sess_val])\n",
    "#     sess_ds[sess+\"_train\"] = Subset(mmser_ds, \n",
    "#                                     indices=list(all_indices-set(sess_dict[sess])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eadf7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_ds(sess_id):\n",
    "#     train_size = int(len(sess_ds[sess_id+\"_train\"])*0.8)\n",
    "#     val_size = len(sess_ds[sess_id+\"_train\"])-train_size\n",
    "#     train_set, val_set = torch.utils.data.random_split(sess_ds[sess_id+\"_train\"], [train_size, val_size])\n",
    "#     test_set = sess_ds[sess_id+\"_test\"]\n",
    "#     # train_set = sess_ds[sess_id+\"_train\"]\n",
    "#     # val_set = sess_ds[sess_id+\"_val\"]\n",
    "\n",
    "#     print(\"Train Samples:\", len(train_set))\n",
    "#     print(\"Val Samples:\", len(val_set))\n",
    "#     print(\"Test Samples:\", len(test_set))\n",
    "    \n",
    "#     return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a781f-52f2-4968-958b-f438c21c6216",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca1b078-b6fe-4432-9329-46bf5b25c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import vidaug.augmentors as va\n",
    "import random\n",
    "\n",
    "class VidaugDataset(Dataset):\n",
    "    \n",
    "    def collate(self, audio, video, max_size=500):\n",
    "        padded_audio = pad_sequence([torch.tensor(a.squeeze()) for a in audio]+[torch.empty(500, 104)], batch_first=True)[:-1]\n",
    "        padded_video = pad_sequence([v.squeeze().clone().detach() for v in video]+[torch.empty(500,88,88)], batch_first=True)[:-1, np.newaxis, : ,:,:]\n",
    "        mask = torch.zeros_like(padded_audio)\n",
    "        mask[padded_audio != 0] = 1\n",
    "        return padded_audio, padded_video, mask\n",
    "    \n",
    "    \n",
    "    def __init__(self, audio_feats_list, \n",
    "                 video_feats_list, \n",
    "                 text_list, \n",
    "                 labels_list):\n",
    "        self.audio_feats_list = audio_feats_list\n",
    "        self.video_feats_list = video_feats_list\n",
    "        self.text_list = text_list\n",
    "        self.labels_list = labels_list\n",
    "    \n",
    "        print(len(self.audio_feats_list))\n",
    "        print(len(self.video_feats_list))\n",
    "        print(len(self.text_list))\n",
    "        print(len(self.labels_list))\n",
    "        \n",
    "        self.origin_len = len(self.labels_list)\n",
    "        self.aug_len = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    def __aug__(self, niters=2, aug_prob=0.3, k=None):\n",
    "        sometimes = lambda aug: va.Sometimes(aug_prob, aug) # Used to apply augmentor with 50% probability\n",
    "        \n",
    "        transform_list = [\n",
    "            sometimes(va.InvertColor()),\n",
    "            sometimes(va.Salt()),\n",
    "            sometimes(va.Pepper()),\n",
    "            sometimes(va.RandomShear(0.2, 0.2)),\n",
    "            sometimes(va.HorizontalFlip()),\n",
    "            sometimes(va.VerticalFlip()),\n",
    "            sometimes(va.RandomRotate(30)),\n",
    "            sometimes(va.GaussianBlur(0.8)),\n",
    "            sometimes(va.ElasticTransformation(0.2,0.2)),\n",
    "            sometimes(va.PiecewiseAffineTransform(20,10,0.5)),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        self.aug_audio_feats_list = []\n",
    "        self.aug_video_feats_list = []\n",
    "        self.aug_text_list = []\n",
    "        self.aug_labels_list = []\n",
    "        \n",
    "        for smp_id in tqdm(range(len(self.video_feats_list))):\n",
    "            for i in range(niters):\n",
    "                if k is None:\n",
    "                    seq = va.Sequential(random.choices(transform_list, \n",
    "                                                   k=random.choice(range(len(transform_list)))))        \n",
    "                else:\n",
    "                    seq = va.Sequential(random.choices(transform_list, \n",
    "                                                   k=k))\n",
    "                vid = self.video_feats_list[smp_id].squeeze()\n",
    "                # change to color \n",
    "                vid = [cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB).astype(np.uint8) for frame in vid]\n",
    "                vid = np.stack(vid)\n",
    "                video_aug = seq(vid)\n",
    "                # print(video_aug[0].shape)\n",
    "                video_aug = [cv2.cvtColor(frame.astype(np.float32), cv2.COLOR_RGB2GRAY).astype(np.uint8) for frame in video_aug]\n",
    "                video_aug = np.stack(video_aug)\n",
    "                \n",
    "                # transform = avhubert_utils.Compose([\n",
    "                #   avhubert_utils.Normalize(0.0, 255.0),\n",
    "                #   avhubert_utils.CenterCrop((88, 88)),\n",
    "                #   avhubert_utils.Normalize(0.421, 0.165)])\n",
    "                # video_aug = transform(video_aug)[np.newaxis, np.newaxis]\n",
    "\n",
    "                video_aug = (video_aug/255)[np.newaxis, np.newaxis]\n",
    "\n",
    "                \n",
    "                video_aug = torch.tensor(video_aug)\n",
    "                self.aug_audio_feats_list.append(self.audio_feats_list[smp_id])\n",
    "                self.aug_video_feats_list.append(video_aug)\n",
    "                self.aug_text_list.append(self.text_list[smp_id])\n",
    "                self.aug_labels_list.append(self.labels_list[smp_id])\n",
    "                \n",
    "        self.aug_len = len(self.aug_labels_list)\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.origin_len + self.aug_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.origin_len:\n",
    "            audio_feats = self.audio_feats_list[idx]\n",
    "            video_feats = self.video_feats_list[idx]\n",
    "            padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
    "            return {\n",
    "                \"padding_mask\": padding_mask[0][:500, :].float(),\n",
    "                \"audio\": padded_audio[0][:500, :].T.float(),\n",
    "                \"video\": padded_video[0][:, :500, :, :].float(),\n",
    "                \"text\": self.text_list[idx],\n",
    "                \"labels\": self.labels_list[idx]\n",
    "            }\n",
    "        else:\n",
    "            idx = idx - self.origin_len\n",
    "            audio_feats = self.aug_audio_feats_list[idx]\n",
    "            video_feats = self.aug_video_feats_list[idx]\n",
    "            padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
    "            return {\n",
    "                \"padding_mask\": padding_mask[0][:500, :].float(),\n",
    "                \"audio\": padded_audio[0][:500, :].T.float(),\n",
    "                \"video\": padded_video[0][:, :500, :, :].float(),\n",
    "                \"text\": self.aug_text_list[idx],\n",
    "                \"labels\": self.aug_labels_list[idx]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f90f1e8f-2234-4722-a4d0-535048f8077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats_list = mmser_ds.audio_feats_list\n",
    "video_feats_list = mmser_ds.video_feats_list\n",
    "text_list = list(meta_df_[\"transcript\"])\n",
    "labels_list = list(meta_df_[\"emotion_id\"])\n",
    "\n",
    "del mmser_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fed7638-bfe1-4fe9-b11a-ef464dcfc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = sess_dict['Ses01']\n",
    "test_indices = sess_dict['Ses02']\n",
    "train_indices = list(all_indices-set(val_indices)-set(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "543c179e-1903-4419-a2fd-c33e9e9c0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3423\n",
      "3423\n",
      "3423\n",
      "3423\n",
      "1085\n",
      "1085\n",
      "1085\n",
      "1085\n",
      "1023\n",
      "1023\n",
      "1023\n",
      "1023\n"
     ]
    }
   ],
   "source": [
    "train_ds = VidaugDataset(\n",
    "    [item.detach().numpy() for i, item in enumerate(audio_feats_list) if i in train_indices],\n",
    "    [item.detach().numpy() for i, item in enumerate(video_feats_list) if i in train_indices],\n",
    "    [item for i, item in enumerate(text_list) if i in train_indices],\n",
    "    [item for i, item in enumerate(labels_list) if i in train_indices]\n",
    ")\n",
    "\n",
    "val_ds = VidaugDataset(\n",
    "    [item.detach().numpy() for i, item in enumerate(audio_feats_list) if i in val_indices],\n",
    "    [item.detach().numpy() for i, item in enumerate(video_feats_list) if i in val_indices],\n",
    "    [item for i, item in enumerate(text_list) if i in val_indices],\n",
    "    [item for i, item in enumerate(labels_list) if i in val_indices]\n",
    ")\n",
    "\n",
    "test_ds = VidaugDataset(\n",
    "    [item.detach().numpy() for i, item in enumerate(audio_feats_list) if i in test_indices],\n",
    "    [item.detach().numpy() for i, item in enumerate(video_feats_list) if i in test_indices],\n",
    "    [item for i, item in enumerate(text_list) if i in test_indices],\n",
    "    [item for i, item in enumerate(labels_list) if i in test_indices]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1c1dc6d-8d9e-4015-9fa4-0b9893a25c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0, 2.0, 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de096d1d-6742-4cb2-aff1-015943b0b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3423/3423 [03:25<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds.__aug__(2, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69a7c46a-08eb-45dc-8b11-f0bbbdb6d6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 88, 88])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][\"video\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573929b",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be847c2-4282-4906-8add-40136adb8012",
   "metadata": {},
   "source": [
    "API: 2999b8f99f0f62b4f64c48a1c8be9a16945183e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2329893c-3888-4ff6-b37e-b1b538afb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: pre-trained w/o fine-tuning\n"
     ]
    }
   ],
   "source": [
    "user_dir = \"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"/home/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecfde834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Ses01 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmmser\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/wandb/run-20231105_101316-k2lh83p4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mmser/av_hubert/runs/k2lh83p4' target=\"_blank\">pleasant-elevator-12</a></strong> to <a href='https://wandb.ai/mmser/av_hubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mmser/av_hubert' target=\"_blank\">https://wandb.ai/mmser/av_hubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mmser/av_hubert/runs/k2lh83p4' target=\"_blank\">https://wandb.ai/mmser/av_hubert/runs/k2lh83p4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ses01\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sess_id = list(sess_dict.keys())[0]\n",
    "print(\"=\"*10, sess_id, \"=\"*10)\n",
    "\n",
    "avhubert_classifier = AVHUBERTClassifier(model, 768, 256)\n",
    "for param in avhubert_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "wandb.init()\n",
    "print(sess_id)\n",
    "# train_set, val_set, test_set = build_ds(sess_id)\n",
    "train_set = train_ds\n",
    "val_set = val_ds\n",
    "test_set = test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "341a9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=os.path.join(\"check_pts\", \"AVHUBERT\", sess_id, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\"))\n",
    "\n",
    "training_args = TrainingArguments(output_dir,report_to=\"wandb\")\n",
    "training_args.remove_unused_columns=False\n",
    "training_args.per_device_train_batch_size=6\n",
    "training_args.per_device_eval_batch_size=6\n",
    "training_args.logging_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.eval_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.evaluation_strategy=\"steps\" \n",
    "training_args.logging_strategy=\"steps\"\n",
    "training_args.load_best_model_at_end=True,\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.learning_rate=5e-4\n",
    "training_args.num_train_epochs=15\n",
    "training_args.metric_for_best_model = 'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27ade4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_trainer import CustomTrainer , compute_metrics\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback, TrainerState\n",
    "\n",
    "avhubert_classifier = avhubert_classifier.to(device)\n",
    "trainer = CustomTrainer(\n",
    "    model=avhubert_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6778b05-02f5-4598-86ed-2a69d88a1323",
   "metadata": {},
   "source": [
    "##### Gradual Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a1b316-8b55-4e88-bbf0-32e4d8e271dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreezingCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, freeze_encoder_epochs: int):\n",
    "        self.freeze_encoder_epochs = freeze_encoder_epochs\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(state.epoch, self.freeze_encoder_epochs)\n",
    "        model = kwargs[\"model\"]\n",
    "        if state.epoch >= self.freeze_encoder_epochs:\n",
    "            print(\"=\"*10, \"Freezing\", \"=\"*10)\n",
    "            for param in model.encoder.feature_extractor_video.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90bf04a5-0861-4dba-917b-3a23fdfff97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing_callback = FreezingCallback(5)\n",
    "# trainer.add_callback(freezing_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f342fd-5ad9-47c6-9858-1b6b721a1f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20534' max='25680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20534/25680 3:19:57 < 50:06, 1.71 it/s, Epoch 11.99/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wa</th>\n",
       "      <th>Ua</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.419200</td>\n",
       "      <td>1.364771</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>1.381700</td>\n",
       "      <td>1.379857</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>1.362629</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>1.385200</td>\n",
       "      <td>1.387437</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.374400</td>\n",
       "      <td>1.387287</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>1.370700</td>\n",
       "      <td>1.362081</td>\n",
       "      <td>0.356682</td>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.193193</td>\n",
       "      <td>0.356682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1162</td>\n",
       "      <td>1.373600</td>\n",
       "      <td>1.358783</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>1.377100</td>\n",
       "      <td>1.372984</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>1.361500</td>\n",
       "      <td>1.368598</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.383400</td>\n",
       "      <td>1.366960</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1826</td>\n",
       "      <td>1.367400</td>\n",
       "      <td>1.360205</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>1.381000</td>\n",
       "      <td>1.377252</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2158</td>\n",
       "      <td>1.378900</td>\n",
       "      <td>1.370967</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2324</td>\n",
       "      <td>1.366800</td>\n",
       "      <td>1.378901</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>1.379600</td>\n",
       "      <td>1.365118</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2656</td>\n",
       "      <td>1.371100</td>\n",
       "      <td>1.357168</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2822</td>\n",
       "      <td>1.377400</td>\n",
       "      <td>1.368240</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2988</td>\n",
       "      <td>1.365500</td>\n",
       "      <td>1.360755</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3154</td>\n",
       "      <td>1.380800</td>\n",
       "      <td>1.367022</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.375100</td>\n",
       "      <td>1.367395</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3486</td>\n",
       "      <td>1.376200</td>\n",
       "      <td>1.364028</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3652</td>\n",
       "      <td>1.374400</td>\n",
       "      <td>1.355822</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3818</td>\n",
       "      <td>1.378200</td>\n",
       "      <td>1.365138</td>\n",
       "      <td>0.363134</td>\n",
       "      <td>0.309634</td>\n",
       "      <td>0.285966</td>\n",
       "      <td>0.363134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3984</td>\n",
       "      <td>1.368900</td>\n",
       "      <td>1.369932</td>\n",
       "      <td>0.353917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.185029</td>\n",
       "      <td>0.353917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.368500</td>\n",
       "      <td>1.368240</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4316</td>\n",
       "      <td>1.375300</td>\n",
       "      <td>1.367688</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4482</td>\n",
       "      <td>1.369700</td>\n",
       "      <td>1.363060</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4648</td>\n",
       "      <td>1.377600</td>\n",
       "      <td>1.367679</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4814</td>\n",
       "      <td>1.371600</td>\n",
       "      <td>1.368223</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>1.376500</td>\n",
       "      <td>1.373212</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5146</td>\n",
       "      <td>1.377100</td>\n",
       "      <td>1.372970</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5312</td>\n",
       "      <td>1.376200</td>\n",
       "      <td>1.367775</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5478</td>\n",
       "      <td>1.371300</td>\n",
       "      <td>1.366748</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5644</td>\n",
       "      <td>1.371500</td>\n",
       "      <td>1.362798</td>\n",
       "      <td>0.292166</td>\n",
       "      <td>0.272164</td>\n",
       "      <td>0.185531</td>\n",
       "      <td>0.292166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5810</td>\n",
       "      <td>1.374100</td>\n",
       "      <td>1.371270</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5976</td>\n",
       "      <td>1.367800</td>\n",
       "      <td>1.362997</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6142</td>\n",
       "      <td>1.372800</td>\n",
       "      <td>1.364836</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6308</td>\n",
       "      <td>1.380200</td>\n",
       "      <td>1.365460</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6474</td>\n",
       "      <td>1.372800</td>\n",
       "      <td>1.362939</td>\n",
       "      <td>0.256221</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>0.256221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>1.344400</td>\n",
       "      <td>1.290380</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.378143</td>\n",
       "      <td>0.212920</td>\n",
       "      <td>0.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6806</td>\n",
       "      <td>1.267600</td>\n",
       "      <td>1.344995</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>0.419340</td>\n",
       "      <td>0.271335</td>\n",
       "      <td>0.368664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6972</td>\n",
       "      <td>1.267200</td>\n",
       "      <td>1.223117</td>\n",
       "      <td>0.401843</td>\n",
       "      <td>0.451659</td>\n",
       "      <td>0.376125</td>\n",
       "      <td>0.401843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7138</td>\n",
       "      <td>1.238100</td>\n",
       "      <td>1.172630</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.443061</td>\n",
       "      <td>0.407008</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7304</td>\n",
       "      <td>1.260400</td>\n",
       "      <td>1.225213</td>\n",
       "      <td>0.388018</td>\n",
       "      <td>0.430457</td>\n",
       "      <td>0.310490</td>\n",
       "      <td>0.388018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>1.210200</td>\n",
       "      <td>1.148844</td>\n",
       "      <td>0.417512</td>\n",
       "      <td>0.449636</td>\n",
       "      <td>0.340291</td>\n",
       "      <td>0.417512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7636</td>\n",
       "      <td>1.217400</td>\n",
       "      <td>1.202932</td>\n",
       "      <td>0.391705</td>\n",
       "      <td>0.430160</td>\n",
       "      <td>0.313909</td>\n",
       "      <td>0.391705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7802</td>\n",
       "      <td>1.231800</td>\n",
       "      <td>1.248311</td>\n",
       "      <td>0.378802</td>\n",
       "      <td>0.408288</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.378802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7968</td>\n",
       "      <td>1.226800</td>\n",
       "      <td>1.269497</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>0.403050</td>\n",
       "      <td>0.293583</td>\n",
       "      <td>0.360369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8134</td>\n",
       "      <td>1.225600</td>\n",
       "      <td>1.188649</td>\n",
       "      <td>0.400922</td>\n",
       "      <td>0.421519</td>\n",
       "      <td>0.329827</td>\n",
       "      <td>0.400922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.212700</td>\n",
       "      <td>1.173060</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.441245</td>\n",
       "      <td>0.320839</td>\n",
       "      <td>0.405530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8466</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>1.213660</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>0.434927</td>\n",
       "      <td>0.339132</td>\n",
       "      <td>0.405530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8632</td>\n",
       "      <td>1.208000</td>\n",
       "      <td>1.178463</td>\n",
       "      <td>0.411060</td>\n",
       "      <td>0.443097</td>\n",
       "      <td>0.330140</td>\n",
       "      <td>0.411060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8798</td>\n",
       "      <td>1.223000</td>\n",
       "      <td>1.184228</td>\n",
       "      <td>0.423963</td>\n",
       "      <td>0.463432</td>\n",
       "      <td>0.399012</td>\n",
       "      <td>0.423963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8964</td>\n",
       "      <td>1.203000</td>\n",
       "      <td>1.228309</td>\n",
       "      <td>0.400922</td>\n",
       "      <td>0.434534</td>\n",
       "      <td>0.335642</td>\n",
       "      <td>0.400922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9130</td>\n",
       "      <td>1.187200</td>\n",
       "      <td>1.183450</td>\n",
       "      <td>0.391705</td>\n",
       "      <td>0.433879</td>\n",
       "      <td>0.311087</td>\n",
       "      <td>0.391705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9296</td>\n",
       "      <td>1.213100</td>\n",
       "      <td>1.147144</td>\n",
       "      <td>0.412903</td>\n",
       "      <td>0.443864</td>\n",
       "      <td>0.337969</td>\n",
       "      <td>0.412903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9462</td>\n",
       "      <td>1.180400</td>\n",
       "      <td>1.247554</td>\n",
       "      <td>0.404608</td>\n",
       "      <td>0.449623</td>\n",
       "      <td>0.314907</td>\n",
       "      <td>0.404608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9628</td>\n",
       "      <td>1.231900</td>\n",
       "      <td>1.188865</td>\n",
       "      <td>0.411982</td>\n",
       "      <td>0.452473</td>\n",
       "      <td>0.329638</td>\n",
       "      <td>0.411982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9794</td>\n",
       "      <td>1.213900</td>\n",
       "      <td>1.189049</td>\n",
       "      <td>0.401843</td>\n",
       "      <td>0.426881</td>\n",
       "      <td>0.327484</td>\n",
       "      <td>0.401843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9960</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1.159891</td>\n",
       "      <td>0.415668</td>\n",
       "      <td>0.449468</td>\n",
       "      <td>0.336462</td>\n",
       "      <td>0.415668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10126</td>\n",
       "      <td>1.217700</td>\n",
       "      <td>1.297676</td>\n",
       "      <td>0.374194</td>\n",
       "      <td>0.423451</td>\n",
       "      <td>0.293972</td>\n",
       "      <td>0.374194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10292</td>\n",
       "      <td>1.170900</td>\n",
       "      <td>1.214463</td>\n",
       "      <td>0.404608</td>\n",
       "      <td>0.445899</td>\n",
       "      <td>0.324224</td>\n",
       "      <td>0.404608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10458</td>\n",
       "      <td>1.235200</td>\n",
       "      <td>1.173114</td>\n",
       "      <td>0.394470</td>\n",
       "      <td>0.441193</td>\n",
       "      <td>0.349208</td>\n",
       "      <td>0.394470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10624</td>\n",
       "      <td>1.197100</td>\n",
       "      <td>1.222249</td>\n",
       "      <td>0.376037</td>\n",
       "      <td>0.438586</td>\n",
       "      <td>0.320786</td>\n",
       "      <td>0.376037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10790</td>\n",
       "      <td>1.211000</td>\n",
       "      <td>1.151948</td>\n",
       "      <td>0.421198</td>\n",
       "      <td>0.438957</td>\n",
       "      <td>0.355550</td>\n",
       "      <td>0.421198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10956</td>\n",
       "      <td>1.172200</td>\n",
       "      <td>1.185485</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.459774</td>\n",
       "      <td>0.360308</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11122</td>\n",
       "      <td>1.199300</td>\n",
       "      <td>1.139391</td>\n",
       "      <td>0.411982</td>\n",
       "      <td>0.446863</td>\n",
       "      <td>0.332067</td>\n",
       "      <td>0.411982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11288</td>\n",
       "      <td>1.210300</td>\n",
       "      <td>1.157667</td>\n",
       "      <td>0.415668</td>\n",
       "      <td>0.445111</td>\n",
       "      <td>0.339660</td>\n",
       "      <td>0.415668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11454</td>\n",
       "      <td>1.189400</td>\n",
       "      <td>1.147876</td>\n",
       "      <td>0.432258</td>\n",
       "      <td>0.462564</td>\n",
       "      <td>0.360472</td>\n",
       "      <td>0.432258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11620</td>\n",
       "      <td>1.192200</td>\n",
       "      <td>1.139419</td>\n",
       "      <td>0.433180</td>\n",
       "      <td>0.451455</td>\n",
       "      <td>0.369358</td>\n",
       "      <td>0.433180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11786</td>\n",
       "      <td>1.191400</td>\n",
       "      <td>1.265398</td>\n",
       "      <td>0.392627</td>\n",
       "      <td>0.440161</td>\n",
       "      <td>0.307594</td>\n",
       "      <td>0.392627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11952</td>\n",
       "      <td>1.159200</td>\n",
       "      <td>1.174425</td>\n",
       "      <td>0.418433</td>\n",
       "      <td>0.449999</td>\n",
       "      <td>0.345659</td>\n",
       "      <td>0.418433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12118</td>\n",
       "      <td>1.178500</td>\n",
       "      <td>1.182094</td>\n",
       "      <td>0.431336</td>\n",
       "      <td>0.461732</td>\n",
       "      <td>0.361115</td>\n",
       "      <td>0.431336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12284</td>\n",
       "      <td>1.178000</td>\n",
       "      <td>1.143554</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.468720</td>\n",
       "      <td>0.389170</td>\n",
       "      <td>0.451613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>1.177000</td>\n",
       "      <td>1.124579</td>\n",
       "      <td>0.464516</td>\n",
       "      <td>0.479396</td>\n",
       "      <td>0.438152</td>\n",
       "      <td>0.464516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12616</td>\n",
       "      <td>1.143300</td>\n",
       "      <td>1.307277</td>\n",
       "      <td>0.429493</td>\n",
       "      <td>0.461785</td>\n",
       "      <td>0.372482</td>\n",
       "      <td>0.429493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12782</td>\n",
       "      <td>1.186000</td>\n",
       "      <td>1.113408</td>\n",
       "      <td>0.456221</td>\n",
       "      <td>0.494702</td>\n",
       "      <td>0.440382</td>\n",
       "      <td>0.456221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12948</td>\n",
       "      <td>1.175500</td>\n",
       "      <td>1.221649</td>\n",
       "      <td>0.439631</td>\n",
       "      <td>0.460354</td>\n",
       "      <td>0.377027</td>\n",
       "      <td>0.439631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13114</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1.147845</td>\n",
       "      <td>0.461751</td>\n",
       "      <td>0.480983</td>\n",
       "      <td>0.397931</td>\n",
       "      <td>0.461751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13280</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.159139</td>\n",
       "      <td>0.434101</td>\n",
       "      <td>0.438049</td>\n",
       "      <td>0.368495</td>\n",
       "      <td>0.434101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13446</td>\n",
       "      <td>1.177400</td>\n",
       "      <td>1.115385</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.442605</td>\n",
       "      <td>0.395632</td>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13612</td>\n",
       "      <td>1.185800</td>\n",
       "      <td>1.164044</td>\n",
       "      <td>0.454378</td>\n",
       "      <td>0.473940</td>\n",
       "      <td>0.400658</td>\n",
       "      <td>0.454378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13778</td>\n",
       "      <td>1.168600</td>\n",
       "      <td>1.117354</td>\n",
       "      <td>0.477419</td>\n",
       "      <td>0.483018</td>\n",
       "      <td>0.430619</td>\n",
       "      <td>0.477419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13944</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>1.228442</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.472423</td>\n",
       "      <td>0.391661</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14110</td>\n",
       "      <td>1.156100</td>\n",
       "      <td>1.093542</td>\n",
       "      <td>0.465438</td>\n",
       "      <td>0.460134</td>\n",
       "      <td>0.414517</td>\n",
       "      <td>0.465438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14276</td>\n",
       "      <td>1.162000</td>\n",
       "      <td>1.179240</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.439881</td>\n",
       "      <td>0.362224</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14442</td>\n",
       "      <td>1.175100</td>\n",
       "      <td>1.170969</td>\n",
       "      <td>0.441475</td>\n",
       "      <td>0.495398</td>\n",
       "      <td>0.392679</td>\n",
       "      <td>0.441475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14608</td>\n",
       "      <td>1.118300</td>\n",
       "      <td>1.048173</td>\n",
       "      <td>0.504147</td>\n",
       "      <td>0.508812</td>\n",
       "      <td>0.481291</td>\n",
       "      <td>0.504147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14774</td>\n",
       "      <td>1.149700</td>\n",
       "      <td>1.061836</td>\n",
       "      <td>0.482949</td>\n",
       "      <td>0.492007</td>\n",
       "      <td>0.423569</td>\n",
       "      <td>0.482949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14940</td>\n",
       "      <td>1.119000</td>\n",
       "      <td>1.095814</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.517297</td>\n",
       "      <td>0.465421</td>\n",
       "      <td>0.483871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15106</td>\n",
       "      <td>1.171700</td>\n",
       "      <td>1.084974</td>\n",
       "      <td>0.482028</td>\n",
       "      <td>0.497223</td>\n",
       "      <td>0.417844</td>\n",
       "      <td>0.482028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15272</td>\n",
       "      <td>1.129800</td>\n",
       "      <td>1.126627</td>\n",
       "      <td>0.473733</td>\n",
       "      <td>0.490579</td>\n",
       "      <td>0.419065</td>\n",
       "      <td>0.473733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15438</td>\n",
       "      <td>1.121000</td>\n",
       "      <td>1.122546</td>\n",
       "      <td>0.466359</td>\n",
       "      <td>0.485586</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.466359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15604</td>\n",
       "      <td>1.136300</td>\n",
       "      <td>1.083024</td>\n",
       "      <td>0.482949</td>\n",
       "      <td>0.503737</td>\n",
       "      <td>0.448646</td>\n",
       "      <td>0.482949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15770</td>\n",
       "      <td>1.145300</td>\n",
       "      <td>1.108598</td>\n",
       "      <td>0.474654</td>\n",
       "      <td>0.502313</td>\n",
       "      <td>0.464804</td>\n",
       "      <td>0.474654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15936</td>\n",
       "      <td>1.102200</td>\n",
       "      <td>1.347718</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.472435</td>\n",
       "      <td>0.385645</td>\n",
       "      <td>0.414747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16102</td>\n",
       "      <td>1.123800</td>\n",
       "      <td>1.094201</td>\n",
       "      <td>0.478341</td>\n",
       "      <td>0.502720</td>\n",
       "      <td>0.458399</td>\n",
       "      <td>0.478341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16268</td>\n",
       "      <td>1.132400</td>\n",
       "      <td>1.086407</td>\n",
       "      <td>0.474654</td>\n",
       "      <td>0.492453</td>\n",
       "      <td>0.464880</td>\n",
       "      <td>0.474654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16434</td>\n",
       "      <td>1.127500</td>\n",
       "      <td>1.062222</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.516544</td>\n",
       "      <td>0.474644</td>\n",
       "      <td>0.497696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.146400</td>\n",
       "      <td>1.069777</td>\n",
       "      <td>0.515207</td>\n",
       "      <td>0.519148</td>\n",
       "      <td>0.495101</td>\n",
       "      <td>0.515207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16766</td>\n",
       "      <td>1.068800</td>\n",
       "      <td>1.140905</td>\n",
       "      <td>0.488479</td>\n",
       "      <td>0.499300</td>\n",
       "      <td>0.459655</td>\n",
       "      <td>0.488479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16932</td>\n",
       "      <td>1.141700</td>\n",
       "      <td>1.122931</td>\n",
       "      <td>0.489401</td>\n",
       "      <td>0.519022</td>\n",
       "      <td>0.472060</td>\n",
       "      <td>0.489401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17098</td>\n",
       "      <td>1.096600</td>\n",
       "      <td>1.092622</td>\n",
       "      <td>0.482949</td>\n",
       "      <td>0.484753</td>\n",
       "      <td>0.456648</td>\n",
       "      <td>0.482949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17264</td>\n",
       "      <td>1.092900</td>\n",
       "      <td>1.133968</td>\n",
       "      <td>0.504147</td>\n",
       "      <td>0.521059</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.504147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17430</td>\n",
       "      <td>1.072000</td>\n",
       "      <td>1.205124</td>\n",
       "      <td>0.473733</td>\n",
       "      <td>0.503341</td>\n",
       "      <td>0.454802</td>\n",
       "      <td>0.473733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17596</td>\n",
       "      <td>1.074600</td>\n",
       "      <td>1.060746</td>\n",
       "      <td>0.513364</td>\n",
       "      <td>0.511446</td>\n",
       "      <td>0.495083</td>\n",
       "      <td>0.513364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17762</td>\n",
       "      <td>1.093500</td>\n",
       "      <td>1.106985</td>\n",
       "      <td>0.475576</td>\n",
       "      <td>0.466697</td>\n",
       "      <td>0.444786</td>\n",
       "      <td>0.475576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17928</td>\n",
       "      <td>1.118700</td>\n",
       "      <td>1.067545</td>\n",
       "      <td>0.513364</td>\n",
       "      <td>0.518764</td>\n",
       "      <td>0.494343</td>\n",
       "      <td>0.513364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18094</td>\n",
       "      <td>1.065100</td>\n",
       "      <td>1.105394</td>\n",
       "      <td>0.510599</td>\n",
       "      <td>0.517768</td>\n",
       "      <td>0.494296</td>\n",
       "      <td>0.510599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18260</td>\n",
       "      <td>1.066800</td>\n",
       "      <td>1.103761</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.521144</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18426</td>\n",
       "      <td>1.083000</td>\n",
       "      <td>1.130204</td>\n",
       "      <td>0.499539</td>\n",
       "      <td>0.520292</td>\n",
       "      <td>0.475279</td>\n",
       "      <td>0.499539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18592</td>\n",
       "      <td>1.040800</td>\n",
       "      <td>1.139438</td>\n",
       "      <td>0.502304</td>\n",
       "      <td>0.504987</td>\n",
       "      <td>0.473714</td>\n",
       "      <td>0.502304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18758</td>\n",
       "      <td>1.025400</td>\n",
       "      <td>1.091223</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>0.534911</td>\n",
       "      <td>0.506976</td>\n",
       "      <td>0.522581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18924</td>\n",
       "      <td>1.019200</td>\n",
       "      <td>1.154096</td>\n",
       "      <td>0.500461</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.482370</td>\n",
       "      <td>0.500461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19090</td>\n",
       "      <td>0.975100</td>\n",
       "      <td>1.143560</td>\n",
       "      <td>0.502304</td>\n",
       "      <td>0.532571</td>\n",
       "      <td>0.484276</td>\n",
       "      <td>0.502304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19256</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>1.050563</td>\n",
       "      <td>0.529032</td>\n",
       "      <td>0.530971</td>\n",
       "      <td>0.525386</td>\n",
       "      <td>0.529032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19422</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>1.184921</td>\n",
       "      <td>0.470046</td>\n",
       "      <td>0.479127</td>\n",
       "      <td>0.424390</td>\n",
       "      <td>0.470046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19588</td>\n",
       "      <td>1.008700</td>\n",
       "      <td>1.132927</td>\n",
       "      <td>0.494931</td>\n",
       "      <td>0.520889</td>\n",
       "      <td>0.479647</td>\n",
       "      <td>0.494931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19754</td>\n",
       "      <td>1.027900</td>\n",
       "      <td>1.160597</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>0.512065</td>\n",
       "      <td>0.480374</td>\n",
       "      <td>0.497696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19920</td>\n",
       "      <td>1.012800</td>\n",
       "      <td>1.142895</td>\n",
       "      <td>0.507834</td>\n",
       "      <td>0.545807</td>\n",
       "      <td>0.484540</td>\n",
       "      <td>0.507834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20086</td>\n",
       "      <td>0.981400</td>\n",
       "      <td>1.147915</td>\n",
       "      <td>0.505069</td>\n",
       "      <td>0.531835</td>\n",
       "      <td>0.493225</td>\n",
       "      <td>0.505069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20252</td>\n",
       "      <td>0.987700</td>\n",
       "      <td>1.238921</td>\n",
       "      <td>0.476498</td>\n",
       "      <td>0.517672</td>\n",
       "      <td>0.461228</td>\n",
       "      <td>0.476498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20418</td>\n",
       "      <td>0.976700</td>\n",
       "      <td>1.139187</td>\n",
       "      <td>0.504147</td>\n",
       "      <td>0.532223</td>\n",
       "      <td>0.495740</td>\n",
       "      <td>0.504147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multi_modal_ser/finetune_encoder/audio_video/avhubert_trainer.py:34: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric_f1 = load_metric(\"f1\")\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n",
      "/tmp/ipykernel_252124/463448419.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_audio, padded_video, padding_mask = self.collate([audio_feats], [torch.tensor(video_feats)])\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73117560-10f3-484d-813e-af68d15761cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53544a-76b0-4ab5-b22b-f4979adfa195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0829d84-db7b-46f5-9363-8cf7e5d57cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bba2c6-d14c-47b4-a81b-c4c56d3ef80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86cada-d000-4da5-bff4-09d1a5254029",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = trainer.predict(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c74dd-72b3-46d4-8e41-94309b0d2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pred_labels = val_preds.predictions.argmax(axis=1)\n",
    "true_labels = val_preds.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b34e23-f64d-41ff-83ba-c1117cdea3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels[10:15])\n",
    "print(true_labels[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1832c-d791-4271-8f75-b9370915e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d2156-d8b1-4b4e-821e-24a4e8df145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(true_labels, pred_labels, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5d337-63ca-443f-95c1-759243d52539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002797a-6c14-432d-b6a9-f73c393a7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [fn[\"fn\"] for fn in train_set]    \n",
    "val_ids = [fn[\"fn\"] for fn in val_set]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caad23-71cd-42b8-94f6-7f0e5f66085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_ids).intersection(set(val_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e45f2-a43c-4056-a049-2d527b979c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef31267",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "test_result = trainer.predict(test_set).metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40875ee9-9164-4545-b53f-dc2a9e210777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83889a1d-fb15-4e01-9690-307e0650526c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da509cb2-d2af-4235-b631-337cb7964263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ca56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_PROJ_PATH = \"/home/freeze/{}/projector\".format(sess_id)\n",
    "FREEZE_CLAS_PATH = \"/home/freeze/{}/classifier\".format(sess_id)\n",
    "os.makedirs(FREEZE_PROJ_PATH, exist_ok=True)\n",
    "os.makedirs(FREEZE_CLAS_PATH, exist_ok=True)\n",
    "\n",
    "FREEZE_PROJ = os.path.join(FREEZE_PROJ_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "FREEZE_CLAS = os.path.join(FREEZE_CLAS_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "\n",
    "torch.save(avhubert_classifier.projector.state_dict(), FREEZE_PROJ)\n",
    "torch.save(avhubert_classifier.classifier.state_dict(), FREEZE_CLAS)\n",
    "\n",
    "avhubert_classifier.projector.load_state_dict(torch.load(FREEZE_PROJ))\n",
    "avhubert_classifier.classifier.load_state_dict(torch.load(FREEZE_CLAS))\n",
    "\n",
    "print(eval_result)\n",
    "print(test_result)\n",
    "\n",
    "json_test = json.dumps(test_result, indent=4)\n",
    "json_eval = json.dumps(eval_result, indent=4)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"{}_eval.json\".format(sess_id), \"w\") as outfile:\n",
    "    outfile.write(json_eval)\n",
    "with open(\"{}_test.json\".format(sess_id), \"w\") as outfile:\n",
    "    outfile.write(json_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967ebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
