{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Prepare Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485ef788-832b-4f9d-ad4d-b43c8ae38e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/ChuakBlurk/vidaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041460cc-7376-4d29-b95d-156e4b21b530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install omegaconf==2.1.1\n",
    "# !pip install hydra-core==1.1.1\n",
    "# !pip install -U numpy==1.23.5\n",
    "# !apt-get update && apt-get install -y python3-opencv\n",
    "# !pip install opencv-python\n",
    "# !pip install scikit-image \n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate -U\n",
    "# !pip install wandb\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6c4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/av_hubert.git\n",
    "\n",
    "# %cd av_hubert\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# !pip install scipy\n",
    "# !pip install sentencepiece\n",
    "# !pip install python_speech_features\n",
    "# !pip install scikit-video\n",
    "\n",
    "# %cd fairseq\n",
    "# !pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e231ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert\n",
      "3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "import cv2\n",
    "import tempfile\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "sys.path.append(\"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\")\n",
    "%cd /home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/\n",
    "import utils as avhubert_utils\n",
    "from argparse import Namespace\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128ecb29-946d-4fae-b806-17946ca63f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Sat Nov  4 15:28:51 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   35C    P8              33W / 350W |      6MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1442ff",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15addb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"/home/check_pts/\")\n",
    "# # !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt -O /home/check_pts/avhubert.pt\n",
    "# !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/clean-pretrain/base_vox_iter4.pt -O /home/check_pts/avhubert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edf8e1",
   "metadata": {},
   "source": [
    "### Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da14d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: pre-trained w/o fine-tuning\n"
     ]
    }
   ],
   "source": [
    "user_dir = \"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"/home/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7d434",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86df1ecf-343d-4464-824b-4adf070a8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e339c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_ds import AVHUBERTDataset\n",
    "mmser_ds = torch.load(\"/home/avhubert_ds2.pt\")\n",
    "mmser_ds.video_path = \"/home/face_raw/\"\n",
    "\n",
    "# outputs = model.extract_finetune(mmser_ds[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4043f023-01f6-4abc-aa9d-665b1ee7fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5531/5531 [12:51<00:00,  7.17it/s]\n"
     ]
    }
   ],
   "source": [
    "mmser_ds.cached = False\n",
    "mmser_ds.__cache__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb06072",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da70288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_classifier import AVHUBERTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d16866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = AVHUBERTClassifier(model, 768, 256, mmser_ds.df_[\"emotion_id\"].nunique())\n",
    "# classifier(**mmser_ds[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d940a",
   "metadata": {},
   "source": [
    "### Build Train Test DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c429bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_ = mmser_ds.df_\n",
    "mmser_ds.df_[\"bigsess\"] = mmser_ds.df_[\"session\"].apply(lambda x: x[:-1])\n",
    "sess_dict = mmser_ds.df_.groupby(\"bigsess\").groups\n",
    "all_indices = set(mmser_ds.df_.index.tolist())\n",
    "\n",
    "# sess_ds = {}\n",
    "# for i in range(1,6):\n",
    "#     sess = \"Ses0{}\".format(i)\n",
    "#     sess_val = \"Ses0{}\".format(i%5+1)\n",
    "#     sess_ds[sess+\"_test\"] = Subset(mmser_ds, \n",
    "#                                     indices=sess_dict[sess])\n",
    "#     # sess_ds[sess+\"_val\"] = Subset(mmser_ds, \n",
    "#     #                                 indices=sess_dict[sess_val])\n",
    "#     sess_ds[sess+\"_train\"] = Subset(mmser_ds, \n",
    "#                                     indices=list(all_indices-set(sess_dict[sess])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eadf7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_ds(sess_id):\n",
    "#     train_size = int(len(sess_ds[sess_id+\"_train\"])*0.8)\n",
    "#     val_size = len(sess_ds[sess_id+\"_train\"])-train_size\n",
    "#     train_set, val_set = torch.utils.data.random_split(sess_ds[sess_id+\"_train\"], [train_size, val_size])\n",
    "#     test_set = sess_ds[sess_id+\"_test\"]\n",
    "#     # train_set = sess_ds[sess_id+\"_train\"]\n",
    "#     # val_set = sess_ds[sess_id+\"_val\"]\n",
    "\n",
    "#     print(\"Train Samples:\", len(train_set))\n",
    "#     print(\"Val Samples:\", len(val_set))\n",
    "#     print(\"Test Samples:\", len(test_set))\n",
    "    \n",
    "#     return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a781f-52f2-4968-958b-f438c21c6216",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ca1b078-b6fe-4432-9329-46bf5b25c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import vidaug.augmentors as va\n",
    "import random\n",
    "\n",
    "class VidaugDataset(Dataset):\n",
    "    \n",
    "    def collate(self, audio, video, max_size=500):\n",
    "        padded_audio = pad_sequence([torch.tensor(a.squeeze()) for a in audio]+[torch.empty(500, 104)], batch_first=True)[:-1]\n",
    "        padded_video = pad_sequence([torch.tensor(v.squeeze()) for v in video]+[torch.empty(500,88,88)], batch_first=True)[:-1, np.newaxis, : ,:,:]\n",
    "        mask = torch.zeros_like(padded_audio)\n",
    "        mask[padded_audio != 0] = 1\n",
    "        return padded_audio, padded_video, mask\n",
    "    \n",
    "    \n",
    "    def __init__(self, audio_feats_list, \n",
    "                 video_feats_list, \n",
    "                 text_list, \n",
    "                 labels_list):\n",
    "        self.audio_feats_list = audio_feats_list\n",
    "        self.video_feats_list = video_feats_list\n",
    "        self.text_list = text_list\n",
    "        self.labels_list = labels_list\n",
    "    \n",
    "        print(len(self.audio_feats_list))\n",
    "        print(len(self.video_feats_list))\n",
    "        print(len(self.text_list))\n",
    "        print(len(self.labels_list))\n",
    "        \n",
    "        self.origin_len = len(self.labels_list)\n",
    "        self.aug_len = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    def __aug__(self, niters=2, aug_prob=0.3, k=None):\n",
    "        sometimes = lambda aug: va.Sometimes(aug_prob, aug) # Used to apply augmentor with 50% probability\n",
    "        \n",
    "        transform_list = [\n",
    "            sometimes(va.InvertColor()),\n",
    "            sometimes(va.Salt()),\n",
    "            sometimes(va.Pepper()),\n",
    "            sometimes(va.RandomShear(0.2, 0.2)),\n",
    "            sometimes(va.HorizontalFlip()),\n",
    "            sometimes(va.VerticalFlip()),\n",
    "            sometimes(va.RandomRotate(30)),\n",
    "            sometimes(va.GaussianBlur(0.8)),\n",
    "            sometimes(va.ElasticTransformation(0.2,0.2)),\n",
    "            sometimes(va.PiecewiseAffineTransform(20,10,0.5)),\n",
    "        ]\n",
    "        \n",
    "        if k is None:\n",
    "            seq = va.Sequential(random.choices(transform_list, \n",
    "                                           k=random.choice(range(len(transform_list)))))\n",
    "        \n",
    "        else:\n",
    "            seq = va.Sequential(random.choices(transform_list, \n",
    "                                           k=k))\n",
    "        \n",
    "        self.aug_audio_feats_list = []\n",
    "        self.aug_video_feats_list = []\n",
    "        self.aug_text_list = []\n",
    "        self.aug_labels_list = []\n",
    "        \n",
    "        for smp_id in tqdm(range(len(self.video_feats_list))):\n",
    "            for i in range(niters):\n",
    "                vid = self.video_feats_list[smp_id].squeeze()\n",
    "                # change to color \n",
    "                vid = [cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB).astype(np.uint8) for frame in vid]\n",
    "                vid = np.stack(vid)\n",
    "                video_aug = seq(vid)\n",
    "                video_aug = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY).astype(np.uint8) for frame in video_aug]\n",
    "                video_aug = np.stack(video_aug)\n",
    "                \n",
    "                # transform = avhubert_utils.Compose([\n",
    "                #   avhubert_utils.Normalize(0.0, 255.0),\n",
    "                #   avhubert_utils.CenterCrop((88, 88)),\n",
    "                #   avhubert_utils.Normalize(0.421, 0.165)])\n",
    "                # video_aug = transform(video_aug)[np.newaxis, np.newaxis]\n",
    "\n",
    "                video_aug = (video_aug/255)[np.newaxis, np.newaxis]\n",
    "\n",
    "                \n",
    "                video_aug = torch.tensor(video_aug)\n",
    "                self.aug_audio_feats_list.append(self.audio_feats_list[smp_id])\n",
    "                self.aug_video_feats_list.append(video_aug)\n",
    "                self.aug_text_list.append(self.text_list[smp_id])\n",
    "                self.aug_labels_list.append(self.labels_list[smp_id])\n",
    "                \n",
    "        self.aug_len = len(self.aug_labels_list)\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.origin_len + self.aug_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.origin_len:\n",
    "            audio_feats = self.audio_feats_list[idx]\n",
    "            video_feats = self.video_feats_list[idx]\n",
    "            padded_audio, padded_video, padding_mask = self.collate([audio_feats], [video_feats])\n",
    "            return {\n",
    "                \"padding_mask\": padding_mask[0][:500, :].float(),\n",
    "                \"audio\": padded_audio[0][:500, :].T.float(),\n",
    "                \"video\": padded_video[0][:, :500, :, :].float(),\n",
    "                \"text\": self.text_list[idx].float(),\n",
    "                \"labels\": self.labels_list[idx].float()\n",
    "            }\n",
    "        else:\n",
    "            idx = idx - self.origin_len\n",
    "            audio_feats = self.aug_audio_feats_list[idx]\n",
    "            video_feats = self.aug_video_feats_list[idx]\n",
    "            padded_audio, padded_video, padding_mask = self.collate([audio_feats], [video_feats])\n",
    "            return {\n",
    "                \"padding_mask\": padding_mask[0][:500, :].float(),\n",
    "                \"audio\": padded_audio[0][:500, :].T.float(),\n",
    "                \"video\": padded_video[0][:, :500, :, :].float(),\n",
    "                \"text\": self.aug_text_list[idx].float(),\n",
    "                \"labels\": self.aug_labels_list[idx].float()\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f90f1e8f-2234-4722-a4d0-535048f8077f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mmser_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_feats_list \u001b[38;5;241m=\u001b[39m \u001b[43mmmser_ds\u001b[49m\u001b[38;5;241m.\u001b[39maudio_feats_list\n\u001b[1;32m      2\u001b[0m video_feats_list \u001b[38;5;241m=\u001b[39m mmser_ds\u001b[38;5;241m.\u001b[39mvideo_feats_list\n\u001b[1;32m      3\u001b[0m text_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(meta_df_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mmser_ds' is not defined"
     ]
    }
   ],
   "source": [
    "audio_feats_list = mmser_ds.audio_feats_list\n",
    "video_feats_list = mmser_ds.video_feats_list\n",
    "text_list = list(meta_df_[\"transcript\"])\n",
    "labels_list = list(meta_df_[\"emotion_id\"])\n",
    "\n",
    "del mmser_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fed7638-bfe1-4fe9-b11a-ef464dcfc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = sess_dict['Ses01']\n",
    "test_indices = sess_dict['Ses02']\n",
    "train_indices = list(all_indices-set(val_indices)-set(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "543c179e-1903-4419-a2fd-c33e9e9c0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3423\n",
      "3423\n",
      "3423\n",
      "3423\n",
      "1085\n",
      "1085\n",
      "1085\n",
      "1085\n",
      "1023\n",
      "1023\n",
      "1023\n",
      "1023\n"
     ]
    }
   ],
   "source": [
    "train_ds = VidaugDataset(\n",
    "    [item.detach().numpy() for i, item in enumerate(audio_feats_list) if i in train_indices],\n",
    "    [item.detach().numpy() for i, item in enumerate(video_feats_list) if i in train_indices],\n",
    "    [item for i, item in enumerate(text_list) if i in train_indices],\n",
    "    [item for i, item in enumerate(labels_list) if i in train_indices]\n",
    ")\n",
    "\n",
    "val_ds = VidaugDataset(\n",
    "    [item.detach().numpy() for i, item in enumerate(audio_feats_list) if i in val_indices],\n",
    "    [item.detach().numpy() for i, item in enumerate(video_feats_list) if i in val_indices],\n",
    "    [item for i, item in enumerate(text_list) if i in val_indices],\n",
    "    [item for i, item in enumerate(labels_list) if i in val_indices]\n",
    ")\n",
    "\n",
    "test_ds = VidaugDataset(\n",
    "    [item.detach().numpy() for i, item in enumerate(audio_feats_list) if i in test_indices],\n",
    "    [item.detach().numpy() for i, item in enumerate(video_feats_list) if i in test_indices],\n",
    "    [item for i, item in enumerate(text_list) if i in test_indices],\n",
    "    [item for i, item in enumerate(labels_list) if i in test_indices]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de096d1d-6742-4cb2-aff1-015943b0b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2696/3423 [01:24<00:31, 23.15it/s]"
     ]
    }
   ],
   "source": [
    "train_ds.__aug__(2, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7c46a-08eb-45dc-8b11-f0bbbdb6d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0][\"video\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573929b",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be847c2-4282-4906-8add-40136adb8012",
   "metadata": {},
   "source": [
    "API: 2999b8f99f0f62b4f64c48a1c8be9a16945183e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329893c-3888-4ff6-b37e-b1b538afb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dir = \"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"/home/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfde834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sess_id = list(sess_dict.keys())[0]\n",
    "print(\"=\"*10, sess_id, \"=\"*10)\n",
    "\n",
    "avhubert_classifier = AVHUBERTClassifier(model, 768, 256, 5)\n",
    "for param in avhubert_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "wandb.init()\n",
    "print(sess_id)\n",
    "# train_set, val_set, test_set = build_ds(sess_id)\n",
    "train_set = train_ds\n",
    "val_set = val_ds\n",
    "test_set = test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=os.path.join(\"check_pts\", \"AVHUBERT\", sess_id, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\"))\n",
    "\n",
    "training_args = TrainingArguments(output_dir,report_to=\"wandb\")\n",
    "training_args.remove_unused_columns=False\n",
    "training_args.per_device_train_batch_size=6\n",
    "training_args.per_device_eval_batch_size=6\n",
    "training_args.logging_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.eval_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.evaluation_strategy=\"steps\" \n",
    "training_args.logging_strategy=\"steps\"\n",
    "training_args.load_best_model_at_end=True,\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.learning_rate=5e-3\n",
    "training_args.num_train_epochs=15\n",
    "training_args.metric_for_best_model = 'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ade4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_trainer import CustomTrainer , compute_metrics\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback, TrainerState\n",
    "\n",
    "avhubert_classifier = avhubert_classifier.to(device)\n",
    "trainer = CustomTrainer(\n",
    "    model=avhubert_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6778b05-02f5-4598-86ed-2a69d88a1323",
   "metadata": {},
   "source": [
    "##### Gradual Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1b316-8b55-4e88-bbf0-32e4d8e271dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreezingCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, freeze_encoder_epochs: int):\n",
    "        self.freeze_encoder_epochs = freeze_encoder_epochs\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(state.epoch, self.freeze_encoder_epochs)\n",
    "        model = kwargs[\"model\"]\n",
    "        if state.epoch >= self.freeze_encoder_epochs:\n",
    "            print(\"=\"*10, \"Freezing\", \"=\"*10)\n",
    "            for param in model.encoder.feature_extractor_video.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf04a5-0861-4dba-917b-3a23fdfff97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freezing_callback = FreezingCallback(5)\n",
    "trainer.add_callback(freezing_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f342fd-5ad9-47c6-9858-1b6b721a1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73117560-10f3-484d-813e-af68d15761cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53544a-76b0-4ab5-b22b-f4979adfa195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0829d84-db7b-46f5-9363-8cf7e5d57cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bba2c6-d14c-47b4-a81b-c4c56d3ef80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86cada-d000-4da5-bff4-09d1a5254029",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = trainer.predict(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c74dd-72b3-46d4-8e41-94309b0d2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pred_labels = val_preds.predictions.argmax(axis=1)\n",
    "true_labels = val_preds.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b34e23-f64d-41ff-83ba-c1117cdea3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels[10:15])\n",
    "print(true_labels[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a1832c-d791-4271-8f75-b9370915e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d2156-d8b1-4b4e-821e-24a4e8df145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(true_labels, pred_labels, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5d337-63ca-443f-95c1-759243d52539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002797a-6c14-432d-b6a9-f73c393a7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [fn[\"fn\"] for fn in train_set]    \n",
    "val_ids = [fn[\"fn\"] for fn in val_set]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caad23-71cd-42b8-94f6-7f0e5f66085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_ids).intersection(set(val_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e45f2-a43c-4056-a049-2d527b979c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef31267",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "test_result = trainer.predict(test_set).metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40875ee9-9164-4545-b53f-dc2a9e210777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83889a1d-fb15-4e01-9690-307e0650526c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da509cb2-d2af-4235-b631-337cb7964263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ca56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_PROJ_PATH = \"/home/freeze/{}/projector\".format(sess_id)\n",
    "FREEZE_CLAS_PATH = \"/home/freeze/{}/classifier\".format(sess_id)\n",
    "os.makedirs(FREEZE_PROJ_PATH, exist_ok=True)\n",
    "os.makedirs(FREEZE_CLAS_PATH, exist_ok=True)\n",
    "\n",
    "FREEZE_PROJ = os.path.join(FREEZE_PROJ_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "FREEZE_CLAS = os.path.join(FREEZE_CLAS_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "\n",
    "torch.save(avhubert_classifier.projector.state_dict(), FREEZE_PROJ)\n",
    "torch.save(avhubert_classifier.classifier.state_dict(), FREEZE_CLAS)\n",
    "\n",
    "avhubert_classifier.projector.load_state_dict(torch.load(FREEZE_PROJ))\n",
    "avhubert_classifier.classifier.load_state_dict(torch.load(FREEZE_CLAS))\n",
    "\n",
    "print(eval_result)\n",
    "print(test_result)\n",
    "\n",
    "json_test = json.dumps(test_result, indent=4)\n",
    "json_eval = json.dumps(eval_result, indent=4)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"{}_eval.json\".format(sess_id), \"w\") as outfile:\n",
    "    outfile.write(json_eval)\n",
    "with open(\"{}_test.json\".format(sess_id), \"w\") as outfile:\n",
    "    outfile.write(json_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967ebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
