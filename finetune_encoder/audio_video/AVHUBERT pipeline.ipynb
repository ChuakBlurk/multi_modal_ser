{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Prepare Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6c4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/av_hubert.git\n",
    "\n",
    "# %cd av_hubert\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# !pip install scipy\n",
    "# !pip install sentencepiece\n",
    "# !pip install python_speech_features\n",
    "# !pip install scikit-video\n",
    "\n",
    "# %cd fairseq\n",
    "# !python -m pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e231ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\university\\FYT\\repos\\multi_modal_ser\\finetune_encoder\\audio_video\\av_hubert\\avhubert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1 (default, Oct 28 2018, 08:39:03) [MSC v.1912 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "%cd E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/fairseq\")\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "import cv2\n",
    "import tempfile\n",
    "import torch\n",
    "import utils as avhubert_utils\n",
    "from argparse import Namespace\n",
    "from IPython.display import HTML\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import logfbank\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7624284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacker(feats, stack_order):\n",
    "    \"\"\"\n",
    "    Concatenating consecutive audio frames\n",
    "    Args:\n",
    "    feats - numpy.ndarray of shape [T, F]\n",
    "    stack_order - int (number of neighboring frames to concatenate\n",
    "    Returns:\n",
    "    feats - numpy.ndarray of shape [T', F']\n",
    "    \"\"\"\n",
    "    feat_dim = feats.shape[1]\n",
    "    if len(feats) % stack_order != 0:\n",
    "        res = stack_order - len(feats) % stack_order\n",
    "        res = np.zeros([res, feat_dim]).astype(feats.dtype)\n",
    "        feats = np.concatenate([feats, res], axis=0)\n",
    "    feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1442ff",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15addb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt -O E:/check_pts/avhubert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edf8e1",
   "metadata": {},
   "source": [
    "### Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da14d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: fine-tuned\n"
     ]
    }
   ],
   "source": [
    "user_dir = \"E:/university/FYT/repos/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"E:/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2d7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 104)\n",
      "torch.Size([1, 1, 186, 88, 88])\n"
     ]
    }
   ],
   "source": [
    "smp_id = \"Ses01F_impro05_M017\"\n",
    "video_path = \"E:/datasets/preprocessed/face_raw/{}.mp4\".format(smp_id)\n",
    "audio_path = \"E:/datasets/preprocessed/spectrogram/raw/{}.wav\".format(smp_id)\n",
    "\n",
    "# Load Video\n",
    "# Mute transform first\n",
    "transform = avhubert_utils.Compose([\n",
    "  avhubert_utils.Normalize(0.0, 255.0),\n",
    "  avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
    "  avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
    "\n",
    "frames = avhubert_utils.load_video(video_path)\n",
    "# frames = transform(frames)\n",
    "frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "video_feats = frames\n",
    "\n",
    "# Load Audio\n",
    "sample_rate, wav_data = wavfile.read(audio_path)\n",
    "in_features = model.feature_extractor_audio.proj.in_features\n",
    "assert sample_rate == 16_000 and len(wav_data.shape) == 1\n",
    "audio_feats = logfbank(wav_data, samplerate=sample_rate).astype(np.float32) # [T, F]\n",
    "audio_feats = stacker(audio_feats, in_features//26) # [T/stack_order_audio, F*stack_order_audio]\n",
    "\n",
    "# Match Audio Video\n",
    "print(audio_feats.shape)\n",
    "print(video_feats.shape)\n",
    "\n",
    "diff = audio_feats.shape[0] - video_feats.shape[2]\n",
    "if diff < 0:\n",
    "    audio_feats = np.concatenate([audio_feats, np.zeros([-diff, audio_feats.shape[-1]], dtype=audio_feats.dtype)])\n",
    "elif diff > 0:\n",
    "    audio_feats = audio_feats[:-diff]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "audio_feats = torch.from_numpy(audio_feats.astype(np.float32)).T\n",
    "audio_feats = F.layer_norm(audio_feats, audio_feats.shape[1:])\n",
    "audio_feats = audio_feats.unsqueeze(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa7321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video feature shape: torch.Size([186, 768])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Specify output_layer if you want to extract feature of an intermediate layer\n",
    "    feature, _ = model.extract_finetune(source={'video': video_feats , \n",
    "                                                'audio': audio_feats}, padding_mask=None, output_layer=None)\n",
    "    feature = feature.squeeze(dim=0) \n",
    "print(f\"Video feature shape: {feature.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c90a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Model Standalone Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4e65941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "class MMSERDataset(Dataset):\n",
    "    \"\"\"multi model ser dataset.\"\"\"\n",
    "    \n",
    "    def stacker(self, feats, stack_order):\n",
    "        \"\"\"\n",
    "        Concatenating consecutive audio frames\n",
    "        Args:\n",
    "        feats - numpy.ndarray of shape [T, F]\n",
    "        stack_order - int (number of neighboring frames to concatenate\n",
    "        Returns:\n",
    "        feats - numpy.ndarray of shape [T', F']\n",
    "        \"\"\"\n",
    "        feat_dim = feats.shape[1]\n",
    "        if len(feats) % stack_order != 0:\n",
    "            res = stack_order - len(feats) % stack_order\n",
    "            res = np.zeros([res, feat_dim]).astype(feats.dtype)\n",
    "            feats = np.concatenate([feats, res], axis=0)\n",
    "        feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)\n",
    "        return feats\n",
    "        \n",
    "    def __load_label__(self, cutmap_path):\n",
    "        sheet_df = pd.DataFrame()\n",
    "        for ses in range(1, 6):\n",
    "            extractionmapPATH = cutmap_path + \\\n",
    "                str(ses)+'.xlsx'\n",
    "            xl = pd.ExcelFile(extractionmapPATH)\n",
    "            sheets = xl.sheet_names\n",
    "            for sheet in sheets:\n",
    "                sheet_df = pd.concat([sheet_df, xl.parse(sheet)])\n",
    "        self.df_ = sheet_df\n",
    "        \n",
    "        # remove samples not agreed\n",
    "        self.df_ = pd.merge(self.df_, self.df_text, on=[\"smp_id\"])\n",
    "        self.df_[\"emotion_id\"] = self.df_[\"emotion\"].map(self.emo2id)\n",
    "        self.df_ = self.df_[self.df_[\"emotion_id\"].notna()].reset_index(drop=True)\n",
    "        self.df_[\"session\"] = self.df_[\"smp_id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "        self.df_ = self.df_[self.df_[\"smp_id\"].str.startswith(\"Ses01F_impro\")].reset_index(drop=True)\n",
    "        \n",
    "    def __load_text__(self, text_path):\n",
    "        self.df_text = pd.read_csv(text_path)\n",
    "        pass\n",
    "    \n",
    "    def __load_audio__(self, fn_path):\n",
    "        self.fn_list = list(self.df_[\"smp_id\"])\n",
    "        self.raw_list = []\n",
    "        for fn in self.fn_list:\n",
    "            self.raw_list.append(wavfile.read(os.path.join(fn_path, fn)+'.wav')[1])\n",
    "    \n",
    "    def __load_video__(self, idx):\n",
    "        frames = avhubert_utils.load_video(os.path.join(self.video_path, idx+\".mp4\"))\n",
    "#         transform = avhubert_utils.Compose([\n",
    "#           avhubert_utils.Normalize(0.0, 255.0),\n",
    "#           avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
    "#           avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
    "\n",
    "        frames = avhubert_utils.load_video(video_path)\n",
    "        # frames = transform(frames)\n",
    "        frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "        video_feats = frames\n",
    "        return video_feats\n",
    "    \n",
    "    def __init__(self, \n",
    "                 fn_path, \n",
    "                 cutmap_path, \n",
    "                 text_path, \n",
    "                 video_path, \n",
    "                 emo2id,\n",
    "                 audio_in_features = 104):\n",
    "        \n",
    "        self.emo2id = emo2id\n",
    "        self.audio_in_features = audio_in_features\n",
    "        self.video_path = video_path\n",
    "        self.__load_text__(text_path)\n",
    "        self.__load_label__(cutmap_path)\n",
    "        self.__load_audio__(fn_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df_.shape[0]\n",
    "    \n",
    "    def __getsingle__(self, idx):\n",
    "        raw_audio = self.raw_list[idx]\n",
    "        video_feats = self.__load_video__(self.fn_list[idx])\n",
    "        audio_feats = logfbank(raw_audio, samplerate=AUDIORATE).astype(np.float32) # [T, F]\n",
    "        audio_feats = self.stacker(audio_feats, self.audio_in_features//26) # [T/stack_order_audio, F*stack_order_audio]\n",
    "\n",
    "        diff = audio_feats.shape[0] - video_feats.shape[2]\n",
    "        if diff < 0:\n",
    "            audio_feats = np.concatenate([audio_feats, np.zeros([-diff, audio_feats.shape[-1]], dtype=audio_feats.dtype)])\n",
    "        elif diff > 0:\n",
    "            audio_feats = audio_feats[:-diff]\n",
    "\n",
    "        audio_feats = torch.from_numpy(audio_feats.astype(np.float32)).T\n",
    "        audio_feats = F.layer_norm(audio_feats, audio_feats.shape[1:])\n",
    "        audio_feats = audio_feats.unsqueeze(dim=0)\n",
    "        return audio_feats, video_feats\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return [self.__getitem__(i) for i in range(*idx.indices(len(self)))]  # type: ignore\n",
    "        else:\n",
    "            audio_feats, video_feats = self.__getsingle__(idx)\n",
    "\n",
    "            return {\n",
    "                \"sess\": list(self.df_[\"session\"])[idx],\n",
    "                \"fn\": self.fn_list[idx],\n",
    "                \"audio\": audio_feats,\n",
    "                \"video\": video_feats,\n",
    "                \"text\": list(self.df_[\"transcript\"])[idx],\n",
    "                \"labels\": list(self.df_[\"emotion_id\"])[idx]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e339c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmser_ds = torch.load(\"E:/datasets/preprocessed/dataset/avhubert_ds.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e62f9089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.7083e-01, -1.8842e-02, -3.7301e-02,  ..., -1.2694e-03,\n",
       "            1.6491e-01, -1.7138e-01],\n",
       "          [-2.1259e-02, -7.6889e-02, -1.3348e-01,  ..., -2.9132e-01,\n",
       "            2.7133e-01, -1.0294e-01],\n",
       "          [ 9.6669e-02, -5.6949e-02, -1.4569e-01,  ..., -3.8035e-01,\n",
       "            2.3745e-01,  3.0098e-04],\n",
       "          ...,\n",
       "          [ 1.5377e-01,  1.3366e-01, -1.1035e-01,  ..., -5.0409e-01,\n",
       "            8.9877e-02,  8.1486e-02],\n",
       "          [ 9.9678e-02,  1.1212e-01, -8.9646e-02,  ..., -5.1223e-01,\n",
       "            1.3578e-01,  3.8791e-02],\n",
       "          [-2.7174e-01,  1.2212e-01,  1.2415e-01,  ..., -3.3966e-01,\n",
       "            1.7755e-01, -6.1008e-03]]], grad_fn=<NativeLayerNormBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "AUDIORATE = 16000\n",
    "\n",
    "model.extract_finetune(mmser_ds[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ad7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
