{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d4075b",
   "metadata": {},
   "source": [
    "### Prepare Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041460cc-7376-4d29-b95d-156e4b21b530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install omegaconf==2.1.1\n",
    "# !pip install hydra-core==1.1.1\n",
    "# !pip install -U numpy==1.23.5\n",
    "# !apt-get update && apt-get install -y python3-opencv\n",
    "# !pip install opencv-python\n",
    "# !pip install scikit-image \n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate -U\n",
    "# !pip install wandb\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6c4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/av_hubert.git\n",
    "\n",
    "# %cd av_hubert\n",
    "# !git submodule init\n",
    "# !git submodule update\n",
    "# !pip install scipy\n",
    "# !pip install sentencepiece\n",
    "# !pip install python_speech_features\n",
    "# !pip install scikit-video\n",
    "\n",
    "# %cd fairseq\n",
    "# !pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e231ede0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert\n",
      "3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "import cv2\n",
    "import tempfile\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "sys.path.append(\"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\")\n",
    "%cd /home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/\n",
    "import utils as avhubert_utils\n",
    "from argparse import Namespace\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sys\n",
    "print(sys.version)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128ecb29-946d-4fae-b806-17946ca63f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Sun Oct 22 09:34:31 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:53:00.0 Off |                  N/A |\n",
      "|  0%   46C    P8              44W / 300W |      5MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:8D:00.0 Off |                  N/A |\n",
      "|  0%   46C    P8              22W / 300W |      5MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1442ff",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15addb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"/home/check_pts/\")\n",
    "# !wget https://dl.fbaipublicfiles.com/avhubert/model/lrs3_vox/vsr/base_vox_433h.pt -O /home/check_pts/avhubert.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edf8e1",
   "metadata": {},
   "source": [
    "### Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da14d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: fine-tuned\n"
     ]
    }
   ],
   "source": [
    "user_dir = \"/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/avhubert\"\n",
    "utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "ckpt_path = \"/home/check_pts/avhubert.pt\"\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])  \n",
    "model = models[0]\n",
    "if hasattr(models[0], 'decoder'):\n",
    "    print(f\"Checkpoint: fine-tuned\")\n",
    "    model = models[0].encoder.w2v_model\n",
    "else:\n",
    "    print(f\"Checkpoint: pre-trained w/o fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7d434",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86df1ecf-343d-4464-824b-4adf070a8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e339c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_ds import AVHUBERTDataset\n",
    "mmser_ds = torch.load(\"/home/avhubert_ds2.pt\")\n",
    "mmser_ds.video_path = \"/home/face_raw/\"\n",
    "\n",
    "# outputs = model.extract_finetune(mmser_ds[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4043f023-01f6-4abc-aa9d-665b1ee7fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5531/5531 [08:25<00:00, 10.95it/s]\n"
     ]
    }
   ],
   "source": [
    "mmser_ds.cached = False\n",
    "mmser_ds.__cache__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb06072",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da70288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avhubert_classifier import AVHUBERTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d16866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-0.0373,  0.0067,  0.1036, -0.1225],\n",
       "         [-0.0195,  0.0183,  0.0853, -0.1143],\n",
       "         [-0.0762,  0.0041,  0.1067, -0.1074],\n",
       "         [-0.0652, -0.0080,  0.1014, -0.1166]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = AVHUBERTClassifier(model, 768, 256, mmser_ds.df_[\"emotion_id\"].nunique())\n",
    "classifier(**mmser_ds[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d940a",
   "metadata": {},
   "source": [
    "### Build Train Test DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c429bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_ = mmser_ds.df_\n",
    "sess_dict = mmser_ds.df_.groupby(\"session\").groups\n",
    "all_indices = set(mmser_ds.df_.index.tolist())\n",
    "\n",
    "sess_ds = {}\n",
    "for sess in sess_dict:\n",
    "    sess_ds[sess+\"_train\"] = Subset(mmser_ds, \n",
    "                                    indices=list(all_indices-set(sess_dict[sess])))\n",
    "    sess_ds[sess+\"_test\"] = Subset(mmser_ds, \n",
    "                                    indices=sess_dict[sess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadf7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ds(sess_id):\n",
    "    train_size = int(len(sess_ds[sess_id+\"_train\"])*0.75)\n",
    "    val_size = len(sess_ds[sess_id+\"_train\"])-train_size\n",
    "    train_set, val_set = torch.utils.data.random_split(sess_ds[sess_id+\"_train\"], [train_size, val_size])\n",
    "    test_set = sess_ds[sess_id+\"_test\"]\n",
    "\n",
    "    print(\"Train Samples:\", len(train_set))\n",
    "    print(\"Val Samples:\", len(val_set))\n",
    "    print(\"Test Samples:\", len(test_set))\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573929b",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be847c2-4282-4906-8add-40136adb8012",
   "metadata": {},
   "source": [
    "API: 2999b8f99f0f62b4f64c48a1c8be9a16945183e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecfde834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Ses04M ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmmser\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/multi_modal_ser/finetune_encoder/audio_video/av_hubert/wandb/run-20231022_094310-n8v4f9oc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mmser/av_hubert/runs/n8v4f9oc' target=\"_blank\">lemon-pond-14</a></strong> to <a href='https://wandb.ai/mmser/av_hubert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mmser/av_hubert' target=\"_blank\">https://wandb.ai/mmser/av_hubert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mmser/av_hubert/runs/n8v4f9oc' target=\"_blank\">https://wandb.ai/mmser/av_hubert/runs/n8v4f9oc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ses04M\n",
      "Train Samples: 3789\n",
      "Val Samples: 1264\n",
      "Test Samples: 478\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sess_id = list(sess_dict.keys())[7]\n",
    "print(\"=\"*10, sess_id, \"=\"*10)\n",
    "\n",
    "avhubert_classifier = AVHUBERTClassifier(model, 768, 256, mmser_ds.df_[\"emotion_id\"].nunique())\n",
    "for param in avhubert_classifier.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "wandb.init()\n",
    "print(sess_id)\n",
    "train_set, val_set, test_set = build_ds(sess_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "341a9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=os.path.join(\"check_pts\", \"AVHUBERT\", sess_id, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\"))\n",
    "\n",
    "training_args = TrainingArguments(output_dir,report_to=\"wandb\")\n",
    "training_args.remove_unused_columns=False\n",
    "training_args.per_device_train_batch_size=24\n",
    "training_args.per_device_eval_batch_size=24\n",
    "training_args.logging_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.eval_steps = int(1000/training_args.per_device_train_batch_size)\n",
    "training_args.evaluation_strategy=\"steps\" \n",
    "training_args.logging_strategy=\"steps\"\n",
    "training_args.load_best_model_at_end=True,\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.learning_rate=7e-4\n",
    "training_args.num_train_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ade4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2830' max='3950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2830/3950 1:28:54 < 35:12, 0.53 it/s, Epoch 35.81/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wa</th>\n",
       "      <th>Ua</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.334300</td>\n",
       "      <td>1.296179</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>0.373608</td>\n",
       "      <td>0.301514</td>\n",
       "      <td>0.370253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.288000</td>\n",
       "      <td>1.255270</td>\n",
       "      <td>0.382911</td>\n",
       "      <td>0.329396</td>\n",
       "      <td>0.311365</td>\n",
       "      <td>0.382911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.235400</td>\n",
       "      <td>1.197959</td>\n",
       "      <td>0.469937</td>\n",
       "      <td>0.453114</td>\n",
       "      <td>0.418037</td>\n",
       "      <td>0.469937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.218100</td>\n",
       "      <td>1.175517</td>\n",
       "      <td>0.472310</td>\n",
       "      <td>0.441990</td>\n",
       "      <td>0.435606</td>\n",
       "      <td>0.472310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.184600</td>\n",
       "      <td>1.181879</td>\n",
       "      <td>0.462025</td>\n",
       "      <td>0.439062</td>\n",
       "      <td>0.446511</td>\n",
       "      <td>0.462025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.180800</td>\n",
       "      <td>1.164788</td>\n",
       "      <td>0.496835</td>\n",
       "      <td>0.496915</td>\n",
       "      <td>0.478611</td>\n",
       "      <td>0.496835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>1.154100</td>\n",
       "      <td>1.146474</td>\n",
       "      <td>0.486551</td>\n",
       "      <td>0.474995</td>\n",
       "      <td>0.459160</td>\n",
       "      <td>0.486551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>1.138400</td>\n",
       "      <td>1.128039</td>\n",
       "      <td>0.510285</td>\n",
       "      <td>0.514040</td>\n",
       "      <td>0.505770</td>\n",
       "      <td>0.510285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>1.152800</td>\n",
       "      <td>1.146407</td>\n",
       "      <td>0.507120</td>\n",
       "      <td>0.502080</td>\n",
       "      <td>0.480332</td>\n",
       "      <td>0.507120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.143000</td>\n",
       "      <td>1.143006</td>\n",
       "      <td>0.500791</td>\n",
       "      <td>0.531895</td>\n",
       "      <td>0.493888</td>\n",
       "      <td>0.500791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>1.141200</td>\n",
       "      <td>1.107620</td>\n",
       "      <td>0.513449</td>\n",
       "      <td>0.511371</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.513449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>1.091900</td>\n",
       "      <td>1.109651</td>\n",
       "      <td>0.519778</td>\n",
       "      <td>0.535364</td>\n",
       "      <td>0.517266</td>\n",
       "      <td>0.519778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>1.100100</td>\n",
       "      <td>1.123507</td>\n",
       "      <td>0.501582</td>\n",
       "      <td>0.509418</td>\n",
       "      <td>0.501093</td>\n",
       "      <td>0.501582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>1.088700</td>\n",
       "      <td>1.154714</td>\n",
       "      <td>0.513449</td>\n",
       "      <td>0.509736</td>\n",
       "      <td>0.484770</td>\n",
       "      <td>0.513449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>1.089100</td>\n",
       "      <td>1.088430</td>\n",
       "      <td>0.523734</td>\n",
       "      <td>0.523212</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.523734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>1.106000</td>\n",
       "      <td>1.128972</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.483160</td>\n",
       "      <td>0.476050</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>1.086700</td>\n",
       "      <td>1.082985</td>\n",
       "      <td>0.539557</td>\n",
       "      <td>0.544947</td>\n",
       "      <td>0.535947</td>\n",
       "      <td>0.539557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>1.077400</td>\n",
       "      <td>1.096981</td>\n",
       "      <td>0.524525</td>\n",
       "      <td>0.540497</td>\n",
       "      <td>0.516284</td>\n",
       "      <td>0.524525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>1.077600</td>\n",
       "      <td>1.087925</td>\n",
       "      <td>0.533228</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>0.527979</td>\n",
       "      <td>0.533228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.064800</td>\n",
       "      <td>1.092733</td>\n",
       "      <td>0.547468</td>\n",
       "      <td>0.552553</td>\n",
       "      <td>0.539136</td>\n",
       "      <td>0.547468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>1.081700</td>\n",
       "      <td>1.091191</td>\n",
       "      <td>0.528481</td>\n",
       "      <td>0.554802</td>\n",
       "      <td>0.522117</td>\n",
       "      <td>0.528481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>1.046400</td>\n",
       "      <td>1.091237</td>\n",
       "      <td>0.530063</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>0.524768</td>\n",
       "      <td>0.530063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>1.072900</td>\n",
       "      <td>1.102523</td>\n",
       "      <td>0.537184</td>\n",
       "      <td>0.563788</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.537184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>1.071100</td>\n",
       "      <td>1.093781</td>\n",
       "      <td>0.518196</td>\n",
       "      <td>0.543705</td>\n",
       "      <td>0.506236</td>\n",
       "      <td>0.518196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.075800</td>\n",
       "      <td>1.062319</td>\n",
       "      <td>0.556171</td>\n",
       "      <td>0.565262</td>\n",
       "      <td>0.553027</td>\n",
       "      <td>0.556171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1066</td>\n",
       "      <td>1.037800</td>\n",
       "      <td>1.058681</td>\n",
       "      <td>0.550633</td>\n",
       "      <td>0.556093</td>\n",
       "      <td>0.546351</td>\n",
       "      <td>0.550633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>1.056047</td>\n",
       "      <td>0.543513</td>\n",
       "      <td>0.549518</td>\n",
       "      <td>0.542214</td>\n",
       "      <td>0.543513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>1.073800</td>\n",
       "      <td>1.064320</td>\n",
       "      <td>0.552215</td>\n",
       "      <td>0.541636</td>\n",
       "      <td>0.547787</td>\n",
       "      <td>0.552215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189</td>\n",
       "      <td>1.046100</td>\n",
       "      <td>1.136302</td>\n",
       "      <td>0.494462</td>\n",
       "      <td>0.539399</td>\n",
       "      <td>0.479468</td>\n",
       "      <td>0.494462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>1.083263</td>\n",
       "      <td>0.543513</td>\n",
       "      <td>0.571347</td>\n",
       "      <td>0.537665</td>\n",
       "      <td>0.543513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1271</td>\n",
       "      <td>1.021500</td>\n",
       "      <td>1.075550</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.567231</td>\n",
       "      <td>0.552076</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>1.069300</td>\n",
       "      <td>1.140195</td>\n",
       "      <td>0.513449</td>\n",
       "      <td>0.533754</td>\n",
       "      <td>0.505866</td>\n",
       "      <td>0.513449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1353</td>\n",
       "      <td>1.057100</td>\n",
       "      <td>1.075820</td>\n",
       "      <td>0.553797</td>\n",
       "      <td>0.578686</td>\n",
       "      <td>0.546854</td>\n",
       "      <td>0.553797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1394</td>\n",
       "      <td>1.050900</td>\n",
       "      <td>1.092109</td>\n",
       "      <td>0.534810</td>\n",
       "      <td>0.567911</td>\n",
       "      <td>0.526910</td>\n",
       "      <td>0.534810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>1.048100</td>\n",
       "      <td>1.076333</td>\n",
       "      <td>0.545095</td>\n",
       "      <td>0.569267</td>\n",
       "      <td>0.539110</td>\n",
       "      <td>0.545095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1476</td>\n",
       "      <td>1.010300</td>\n",
       "      <td>1.062570</td>\n",
       "      <td>0.567247</td>\n",
       "      <td>0.573140</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>0.567247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1517</td>\n",
       "      <td>1.025700</td>\n",
       "      <td>1.077634</td>\n",
       "      <td>0.542722</td>\n",
       "      <td>0.570123</td>\n",
       "      <td>0.537059</td>\n",
       "      <td>0.542722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1558</td>\n",
       "      <td>1.037800</td>\n",
       "      <td>1.092018</td>\n",
       "      <td>0.535601</td>\n",
       "      <td>0.569989</td>\n",
       "      <td>0.527190</td>\n",
       "      <td>0.535601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>1.035700</td>\n",
       "      <td>1.046329</td>\n",
       "      <td>0.561709</td>\n",
       "      <td>0.565699</td>\n",
       "      <td>0.559044</td>\n",
       "      <td>0.561709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.027100</td>\n",
       "      <td>1.040054</td>\n",
       "      <td>0.545095</td>\n",
       "      <td>0.557832</td>\n",
       "      <td>0.541758</td>\n",
       "      <td>0.545095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1681</td>\n",
       "      <td>1.016100</td>\n",
       "      <td>1.038127</td>\n",
       "      <td>0.568829</td>\n",
       "      <td>0.573078</td>\n",
       "      <td>0.567701</td>\n",
       "      <td>0.568829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>1.014900</td>\n",
       "      <td>1.053749</td>\n",
       "      <td>0.548259</td>\n",
       "      <td>0.571029</td>\n",
       "      <td>0.543833</td>\n",
       "      <td>0.548259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1763</td>\n",
       "      <td>1.029200</td>\n",
       "      <td>1.097682</td>\n",
       "      <td>0.543513</td>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.534022</td>\n",
       "      <td>0.543513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804</td>\n",
       "      <td>1.020300</td>\n",
       "      <td>1.073755</td>\n",
       "      <td>0.550633</td>\n",
       "      <td>0.572393</td>\n",
       "      <td>0.543652</td>\n",
       "      <td>0.550633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>1.013000</td>\n",
       "      <td>1.038872</td>\n",
       "      <td>0.559335</td>\n",
       "      <td>0.570113</td>\n",
       "      <td>0.556772</td>\n",
       "      <td>0.559335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1886</td>\n",
       "      <td>1.016500</td>\n",
       "      <td>1.050629</td>\n",
       "      <td>0.553006</td>\n",
       "      <td>0.567597</td>\n",
       "      <td>0.549362</td>\n",
       "      <td>0.553006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1927</td>\n",
       "      <td>1.053700</td>\n",
       "      <td>1.033648</td>\n",
       "      <td>0.560918</td>\n",
       "      <td>0.568488</td>\n",
       "      <td>0.559731</td>\n",
       "      <td>0.560918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>1.050932</td>\n",
       "      <td>0.560127</td>\n",
       "      <td>0.581332</td>\n",
       "      <td>0.555623</td>\n",
       "      <td>0.560127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2009</td>\n",
       "      <td>1.002600</td>\n",
       "      <td>1.052419</td>\n",
       "      <td>0.566456</td>\n",
       "      <td>0.577526</td>\n",
       "      <td>0.562156</td>\n",
       "      <td>0.566456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.019100</td>\n",
       "      <td>1.059886</td>\n",
       "      <td>0.564082</td>\n",
       "      <td>0.576917</td>\n",
       "      <td>0.559614</td>\n",
       "      <td>0.564082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2091</td>\n",
       "      <td>1.027700</td>\n",
       "      <td>1.061895</td>\n",
       "      <td>0.549051</td>\n",
       "      <td>0.577071</td>\n",
       "      <td>0.542247</td>\n",
       "      <td>0.549051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2132</td>\n",
       "      <td>1.006500</td>\n",
       "      <td>1.067965</td>\n",
       "      <td>0.564082</td>\n",
       "      <td>0.574730</td>\n",
       "      <td>0.559348</td>\n",
       "      <td>0.564082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2173</td>\n",
       "      <td>0.987900</td>\n",
       "      <td>1.063570</td>\n",
       "      <td>0.548259</td>\n",
       "      <td>0.575425</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.548259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2214</td>\n",
       "      <td>1.048800</td>\n",
       "      <td>1.059298</td>\n",
       "      <td>0.564082</td>\n",
       "      <td>0.581567</td>\n",
       "      <td>0.558924</td>\n",
       "      <td>0.564082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2255</td>\n",
       "      <td>1.017400</td>\n",
       "      <td>1.048479</td>\n",
       "      <td>0.558544</td>\n",
       "      <td>0.574127</td>\n",
       "      <td>0.557011</td>\n",
       "      <td>0.558544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2296</td>\n",
       "      <td>1.005100</td>\n",
       "      <td>1.074480</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>0.563716</td>\n",
       "      <td>0.535093</td>\n",
       "      <td>0.544304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2337</td>\n",
       "      <td>1.024600</td>\n",
       "      <td>1.047040</td>\n",
       "      <td>0.556171</td>\n",
       "      <td>0.574767</td>\n",
       "      <td>0.552263</td>\n",
       "      <td>0.556171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2378</td>\n",
       "      <td>0.970900</td>\n",
       "      <td>1.092746</td>\n",
       "      <td>0.557753</td>\n",
       "      <td>0.589227</td>\n",
       "      <td>0.548410</td>\n",
       "      <td>0.557753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2419</td>\n",
       "      <td>1.001200</td>\n",
       "      <td>1.059409</td>\n",
       "      <td>0.554589</td>\n",
       "      <td>0.582205</td>\n",
       "      <td>0.548743</td>\n",
       "      <td>0.554589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.019000</td>\n",
       "      <td>1.065342</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.576230</td>\n",
       "      <td>0.552225</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2501</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>1.083397</td>\n",
       "      <td>0.536392</td>\n",
       "      <td>0.573880</td>\n",
       "      <td>0.525232</td>\n",
       "      <td>0.536392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2542</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>1.051843</td>\n",
       "      <td>0.565665</td>\n",
       "      <td>0.579425</td>\n",
       "      <td>0.562311</td>\n",
       "      <td>0.565665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2583</td>\n",
       "      <td>1.005900</td>\n",
       "      <td>1.044374</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.571838</td>\n",
       "      <td>0.562673</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2624</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>1.079252</td>\n",
       "      <td>0.554589</td>\n",
       "      <td>0.581927</td>\n",
       "      <td>0.546869</td>\n",
       "      <td>0.554589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2665</td>\n",
       "      <td>1.004600</td>\n",
       "      <td>1.056164</td>\n",
       "      <td>0.557753</td>\n",
       "      <td>0.582080</td>\n",
       "      <td>0.552450</td>\n",
       "      <td>0.557753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2706</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>1.069610</td>\n",
       "      <td>0.549842</td>\n",
       "      <td>0.575122</td>\n",
       "      <td>0.542042</td>\n",
       "      <td>0.549842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2747</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>1.098239</td>\n",
       "      <td>0.531646</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.518606</td>\n",
       "      <td>0.531646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2788</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>1.048900</td>\n",
       "      <td>0.552215</td>\n",
       "      <td>0.574051</td>\n",
       "      <td>0.549366</td>\n",
       "      <td>0.552215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/27 00:14 < 00:14, 0.92 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multi_modal_ser/finetune_encoder/audio_video/avhubert_trainer.py:34: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric_f1 = load_metric(\"f1\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076f0f70fe2d4f43ada16de14ab16752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7428102ae35f447db8b7abfd42709bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from avhubert_trainer import CustomTrainer , compute_metrics\n",
    "avhubert_classifier = avhubert_classifier.to(device)\n",
    "trainer = CustomTrainer(\n",
    "    model=avhubert_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef31267",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "test_result = trainer.predict(test_set).metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ca56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_PROJ_PATH = \"/home/freeze/{}/projector\".format(sess_id)\n",
    "FREEZE_CLAS_PATH = \"/home/freeze/{}/classifier\".format(sess_id)\n",
    "os.makedirs(FREEZE_PROJ_PATH, exist_ok=True)\n",
    "os.makedirs(FREEZE_CLAS_PATH, exist_ok=True)\n",
    "\n",
    "FREEZE_PROJ = os.path.join(FREEZE_PROJ_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "FREEZE_CLAS = os.path.join(FREEZE_CLAS_PATH, datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\")+\".pt\")\n",
    "\n",
    "torch.save(avhubert_classifier.projector.state_dict(), FREEZE_PROJ)\n",
    "torch.save(avhubert_classifier.classifier.state_dict(), FREEZE_CLAS)\n",
    "\n",
    "model.projector.load_state_dict(torch.load(FREEZE_PROJ))\n",
    "model.classifier.load_state_dict(torch.load(FREEZE_CLAS))\n",
    "\n",
    "print(eval_result)\n",
    "print(test_result)\n",
    "\n",
    "json_test = json.dumps(test_result, indent=4)\n",
    "json_eval = json.dumps(eval_result, indent=4)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"{}_eval.json\".format(sess_id), \"w\") as outfile:\n",
    "    outfile.write(json_eval)\n",
    "with open(\"{}_test.json\".format(sess_id), \"w\") as outfile:\n",
    "    outfile.write(json_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967ebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
