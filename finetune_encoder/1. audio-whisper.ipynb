{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06e5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append(\"E:/university/Year 5 Spring/FYT/code/multi_modal_ser\")\n",
    "from utils.dataset import MMSERDataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0e9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmser_ds = torch.load(\"E:/datasets/preprocessed/dataset/mmser_ds.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78052a",
   "metadata": {},
   "source": [
    "### Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d7d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at openai/whisper-large and are newly initialized: ['model.projector.weight', 'model.projector.bias', 'model.classifier.weight', 'model.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, WhisperForAudioClassification\n",
    "\n",
    "model = WhisperForAudioClassification.from_pretrained(\"openai/whisper-large\").to(device)\n",
    "input_features = mmser_ds[78:79][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c54618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_features).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, axis=1).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82abced",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea498c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions:  ['neu' 'fru' 'ang' 'sad' 'hap' 'sur' 'exc' 'oth' 'fea' 'dis']\n"
     ]
    }
   ],
   "source": [
    "print(\"Emotions: \", mmser_ds.df_[\"emotion\"].unique())\n",
    "model.config.num_labels = mmser_ds.df_[\"emotion\"].nunique()\n",
    "model.classifier = nn.Linear(model.projector.out_features, model.config.num_labels)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aff293",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbcf57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(mmser_ds)*0.7)\n",
    "val_size = int(len(mmser_ds)*0.2)\n",
    "test_size = len(mmser_ds)-int(len(mmser_ds)*0.7)-int(len(mmser_ds)*0.2)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(mmser_ds, [train_size, val_size+test_size])\n",
    "val_set, test_set = torch.utils.data.random_split(val_set, [val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9c8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beef7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=4)\n",
    "val_loader = DataLoader(val_set, batch_size=4)\n",
    "test_loader = DataLoader(test_set, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96c0f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "import datetime\n",
    "\n",
    "training_args = TrainingArguments(output_dir=datetime.datetime.now().date().strftime(format=\"%Y-%m-%d\"))\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        outputs = model(input_features=inputs[\"audio\"].to(device))\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss() # weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b970cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3292eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.logging_steps = 5\n",
    "training_args.remove_unused_columns=False\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b8d37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/fine-tune-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62731652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf50a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser",
   "language": "python",
   "name": "ser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
